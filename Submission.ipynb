{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in test, train and concat the datasets\n",
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "      <th>f31</th>\n",
       "      <th>f32</th>\n",
       "      <th>f33</th>\n",
       "      <th>f34</th>\n",
       "      <th>f35</th>\n",
       "      <th>f36</th>\n",
       "      <th>f37</th>\n",
       "      <th>f38</th>\n",
       "      <th>f39</th>\n",
       "      <th>f40</th>\n",
       "      <th>f41</th>\n",
       "      <th>f42</th>\n",
       "      <th>f43</th>\n",
       "      <th>f44</th>\n",
       "      <th>f45</th>\n",
       "      <th>f46</th>\n",
       "      <th>f47</th>\n",
       "      <th>f48</th>\n",
       "      <th>f49</th>\n",
       "      <th>f50</th>\n",
       "      <th>f51</th>\n",
       "      <th>f52</th>\n",
       "      <th>f53</th>\n",
       "      <th>f54</th>\n",
       "      <th>f55</th>\n",
       "      <th>f56</th>\n",
       "      <th>f57</th>\n",
       "      <th>f58</th>\n",
       "      <th>f59</th>\n",
       "      <th>f60</th>\n",
       "      <th>f61</th>\n",
       "      <th>f62</th>\n",
       "      <th>f63</th>\n",
       "      <th>f64</th>\n",
       "      <th>f65</th>\n",
       "      <th>f66</th>\n",
       "      <th>f67</th>\n",
       "      <th>f68</th>\n",
       "      <th>f69</th>\n",
       "      <th>f70</th>\n",
       "      <th>f71</th>\n",
       "      <th>f72</th>\n",
       "      <th>f73</th>\n",
       "      <th>f74</th>\n",
       "      <th>f75</th>\n",
       "      <th>f76</th>\n",
       "      <th>f77</th>\n",
       "      <th>f78</th>\n",
       "      <th>f79</th>\n",
       "      <th>f80</th>\n",
       "      <th>f81</th>\n",
       "      <th>f82</th>\n",
       "      <th>f83</th>\n",
       "      <th>f84</th>\n",
       "      <th>f85</th>\n",
       "      <th>f86</th>\n",
       "      <th>f87</th>\n",
       "      <th>f88</th>\n",
       "      <th>f89</th>\n",
       "      <th>f90</th>\n",
       "      <th>f91</th>\n",
       "      <th>f92</th>\n",
       "      <th>f93</th>\n",
       "      <th>f94</th>\n",
       "      <th>f95</th>\n",
       "      <th>f96</th>\n",
       "      <th>f97</th>\n",
       "      <th>f98</th>\n",
       "      <th>f99</th>\n",
       "      <th>f100</th>\n",
       "      <th>f101</th>\n",
       "      <th>f102</th>\n",
       "      <th>f103</th>\n",
       "      <th>f104</th>\n",
       "      <th>f105</th>\n",
       "      <th>f106</th>\n",
       "      <th>f107</th>\n",
       "      <th>f108</th>\n",
       "      <th>f109</th>\n",
       "      <th>f110</th>\n",
       "      <th>f111</th>\n",
       "      <th>f112</th>\n",
       "      <th>f113</th>\n",
       "      <th>f114</th>\n",
       "      <th>f115</th>\n",
       "      <th>f116</th>\n",
       "      <th>f117</th>\n",
       "      <th>f118</th>\n",
       "      <th>claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.10859</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>-37.566</td>\n",
       "      <td>0.017364</td>\n",
       "      <td>0.28915</td>\n",
       "      <td>-10.25100</td>\n",
       "      <td>135.12</td>\n",
       "      <td>168900.0</td>\n",
       "      <td>3.992400e+14</td>\n",
       "      <td>86.489</td>\n",
       "      <td>0.59881</td>\n",
       "      <td>1.423200e+09</td>\n",
       "      <td>0.27240</td>\n",
       "      <td>9.455600</td>\n",
       "      <td>-0.050305</td>\n",
       "      <td>1938.300</td>\n",
       "      <td>8.6331</td>\n",
       "      <td>4.0607</td>\n",
       "      <td>26.8670</td>\n",
       "      <td>-1.180</td>\n",
       "      <td>10961.00</td>\n",
       "      <td>1.5397</td>\n",
       "      <td>135.3200</td>\n",
       "      <td>-1.49650</td>\n",
       "      <td>440.080</td>\n",
       "      <td>2.590100e+12</td>\n",
       "      <td>2.194200e+09</td>\n",
       "      <td>2968800.0</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>13.3270</td>\n",
       "      <td>0.75050</td>\n",
       "      <td>18509.0</td>\n",
       "      <td>146820.0</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>1.090600e+16</td>\n",
       "      <td>1705.400</td>\n",
       "      <td>414.29</td>\n",
       "      <td>3.5392</td>\n",
       "      <td>1888.0</td>\n",
       "      <td>0.968930</td>\n",
       "      <td>18.3880</td>\n",
       "      <td>-0.001583</td>\n",
       "      <td>7.7059</td>\n",
       "      <td>5.9325</td>\n",
       "      <td>0.025693</td>\n",
       "      <td>4.5604</td>\n",
       "      <td>0.61122</td>\n",
       "      <td>10.7950</td>\n",
       "      <td>0.341930</td>\n",
       "      <td>0.235010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5237.70</td>\n",
       "      <td>1.29610</td>\n",
       "      <td>163.66</td>\n",
       "      <td>0.403780</td>\n",
       "      <td>0.188600</td>\n",
       "      <td>-0.001446</td>\n",
       "      <td>-0.35416</td>\n",
       "      <td>6.6432</td>\n",
       "      <td>0.30534</td>\n",
       "      <td>0.514020</td>\n",
       "      <td>1.907300e+09</td>\n",
       "      <td>29.861</td>\n",
       "      <td>0.965010</td>\n",
       "      <td>1797.2</td>\n",
       "      <td>72.178</td>\n",
       "      <td>108.6200</td>\n",
       "      <td>1.9799</td>\n",
       "      <td>1.2907</td>\n",
       "      <td>0.99519</td>\n",
       "      <td>1.3228</td>\n",
       "      <td>827.340</td>\n",
       "      <td>7.779900e+14</td>\n",
       "      <td>4.129900e+10</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>6.9835</td>\n",
       "      <td>43956.0</td>\n",
       "      <td>1978.2</td>\n",
       "      <td>5.5084</td>\n",
       "      <td>-0.001081</td>\n",
       "      <td>6.1244</td>\n",
       "      <td>1.231800e+11</td>\n",
       "      <td>275.9200</td>\n",
       "      <td>5308500.0</td>\n",
       "      <td>1704.000</td>\n",
       "      <td>5.022400e+10</td>\n",
       "      <td>53.3980</td>\n",
       "      <td>-2.2012</td>\n",
       "      <td>6871.0</td>\n",
       "      <td>3.8862</td>\n",
       "      <td>-0.00558</td>\n",
       "      <td>5252.100</td>\n",
       "      <td>166.690</td>\n",
       "      <td>1.60740</td>\n",
       "      <td>0.66534</td>\n",
       "      <td>7768.900</td>\n",
       "      <td>0.99662</td>\n",
       "      <td>1.125700e+11</td>\n",
       "      <td>2.2432</td>\n",
       "      <td>0.934160</td>\n",
       "      <td>0.65056</td>\n",
       "      <td>94569.0</td>\n",
       "      <td>21.471</td>\n",
       "      <td>8214.100</td>\n",
       "      <td>0.288010</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>1.412400e+09</td>\n",
       "      <td>0.11093</td>\n",
       "      <td>-12.2280</td>\n",
       "      <td>1.7482</td>\n",
       "      <td>1.90960</td>\n",
       "      <td>-7.11570</td>\n",
       "      <td>4378.80</td>\n",
       "      <td>1.2096</td>\n",
       "      <td>8.613400e+14</td>\n",
       "      <td>140.1</td>\n",
       "      <td>1.01770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.10090</td>\n",
       "      <td>0.299610</td>\n",
       "      <td>11822.000</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>0.45970</td>\n",
       "      <td>-0.83733</td>\n",
       "      <td>1721.90</td>\n",
       "      <td>119810.0</td>\n",
       "      <td>3.874100e+15</td>\n",
       "      <td>9953.600</td>\n",
       "      <td>1.20930</td>\n",
       "      <td>3.334100e+09</td>\n",
       "      <td>0.28631</td>\n",
       "      <td>-0.012858</td>\n",
       "      <td>-0.019912</td>\n",
       "      <td>10.284</td>\n",
       "      <td>6.1872</td>\n",
       "      <td>1.0419</td>\n",
       "      <td>4.6404</td>\n",
       "      <td>31.877</td>\n",
       "      <td>123620.00</td>\n",
       "      <td>1.3951</td>\n",
       "      <td>125.8100</td>\n",
       "      <td>1.19890</td>\n",
       "      <td>136.450</td>\n",
       "      <td>9.098100e+09</td>\n",
       "      <td>4.004100e+10</td>\n",
       "      <td>1564000.0</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>3.1074</td>\n",
       "      <td>1.50330</td>\n",
       "      <td>238000.0</td>\n",
       "      <td>21440.0</td>\n",
       "      <td>-0.001344</td>\n",
       "      <td>3.079400e+16</td>\n",
       "      <td>229.100</td>\n",
       "      <td>844.82</td>\n",
       "      <td>1.4680</td>\n",
       "      <td>4726.5</td>\n",
       "      <td>0.915380</td>\n",
       "      <td>-1.5321</td>\n",
       "      <td>0.982600</td>\n",
       "      <td>7.1112</td>\n",
       "      <td>2.0797</td>\n",
       "      <td>0.042321</td>\n",
       "      <td>4.2523</td>\n",
       "      <td>0.41871</td>\n",
       "      <td>5.4499</td>\n",
       "      <td>0.012737</td>\n",
       "      <td>0.386470</td>\n",
       "      <td>7.3082</td>\n",
       "      <td>283.21</td>\n",
       "      <td>-0.92552</td>\n",
       "      <td>140.80</td>\n",
       "      <td>0.247390</td>\n",
       "      <td>-0.001656</td>\n",
       "      <td>-0.000975</td>\n",
       "      <td>-0.22629</td>\n",
       "      <td>2.4246</td>\n",
       "      <td>0.77147</td>\n",
       "      <td>0.011613</td>\n",
       "      <td>1.803700e+09</td>\n",
       "      <td>64.604</td>\n",
       "      <td>0.262650</td>\n",
       "      <td>4455.0</td>\n",
       "      <td>78.339</td>\n",
       "      <td>745.5100</td>\n",
       "      <td>2.9069</td>\n",
       "      <td>1.4826</td>\n",
       "      <td>1.00510</td>\n",
       "      <td>1.4974</td>\n",
       "      <td>84.446</td>\n",
       "      <td>3.505600e+15</td>\n",
       "      <td>2.242300e+09</td>\n",
       "      <td>0.896300</td>\n",
       "      <td>4.6749</td>\n",
       "      <td>17713.0</td>\n",
       "      <td>9003.1</td>\n",
       "      <td>-4.3546</td>\n",
       "      <td>0.254100</td>\n",
       "      <td>6.9191</td>\n",
       "      <td>1.832400e+11</td>\n",
       "      <td>9.6510</td>\n",
       "      <td>32800.0</td>\n",
       "      <td>1480.600</td>\n",
       "      <td>2.300600e+10</td>\n",
       "      <td>44.0510</td>\n",
       "      <td>205.6900</td>\n",
       "      <td>4295.3</td>\n",
       "      <td>13.3880</td>\n",
       "      <td>0.46843</td>\n",
       "      <td>754.610</td>\n",
       "      <td>83.233</td>\n",
       "      <td>1.18900</td>\n",
       "      <td>29.55000</td>\n",
       "      <td>7343.700</td>\n",
       "      <td>0.99815</td>\n",
       "      <td>4.877700e+13</td>\n",
       "      <td>1.2708</td>\n",
       "      <td>-0.000969</td>\n",
       "      <td>5.29520</td>\n",
       "      <td>6779.0</td>\n",
       "      <td>227.720</td>\n",
       "      <td>34.342</td>\n",
       "      <td>0.340300</td>\n",
       "      <td>0.143370</td>\n",
       "      <td>0.049276</td>\n",
       "      <td>1.903200e+09</td>\n",
       "      <td>0.97673</td>\n",
       "      <td>-56.7580</td>\n",
       "      <td>4.1684</td>\n",
       "      <td>0.34808</td>\n",
       "      <td>4.14200</td>\n",
       "      <td>913.23</td>\n",
       "      <td>1.2464</td>\n",
       "      <td>7.575100e+15</td>\n",
       "      <td>1861.0</td>\n",
       "      <td>0.28359</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.17803</td>\n",
       "      <td>-0.006980</td>\n",
       "      <td>907.270</td>\n",
       "      <td>0.272140</td>\n",
       "      <td>0.45948</td>\n",
       "      <td>0.17327</td>\n",
       "      <td>2298.00</td>\n",
       "      <td>360650.0</td>\n",
       "      <td>1.224500e+13</td>\n",
       "      <td>15827.000</td>\n",
       "      <td>0.38164</td>\n",
       "      <td>1.230300e+09</td>\n",
       "      <td>0.25807</td>\n",
       "      <td>2.455600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.873</td>\n",
       "      <td>7.5463</td>\n",
       "      <td>1.9967</td>\n",
       "      <td>1.9526</td>\n",
       "      <td>817.760</td>\n",
       "      <td>-2948.70</td>\n",
       "      <td>2.0054</td>\n",
       "      <td>1.6826</td>\n",
       "      <td>1.19680</td>\n",
       "      <td>74.624</td>\n",
       "      <td>-3.273900e+10</td>\n",
       "      <td>5.718900e+10</td>\n",
       "      <td>11058.0</td>\n",
       "      <td>-0.003097</td>\n",
       "      <td>8.0241</td>\n",
       "      <td>1.13180</td>\n",
       "      <td>27940.0</td>\n",
       "      <td>862460.0</td>\n",
       "      <td>-0.002207</td>\n",
       "      <td>5.849100e+13</td>\n",
       "      <td>-897.840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3561</td>\n",
       "      <td>3063.4</td>\n",
       "      <td>0.086232</td>\n",
       "      <td>16.1060</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>11.4760</td>\n",
       "      <td>5.3430</td>\n",
       "      <td>0.012162</td>\n",
       "      <td>4.1018</td>\n",
       "      <td>-0.88270</td>\n",
       "      <td>8.1228</td>\n",
       "      <td>-0.676690</td>\n",
       "      <td>0.337700</td>\n",
       "      <td>-1.0732</td>\n",
       "      <td>4097.00</td>\n",
       "      <td>13.45800</td>\n",
       "      <td>159.24</td>\n",
       "      <td>0.322300</td>\n",
       "      <td>0.560090</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>-0.16083</td>\n",
       "      <td>3.5753</td>\n",
       "      <td>0.60970</td>\n",
       "      <td>0.028301</td>\n",
       "      <td>5.271300e+08</td>\n",
       "      <td>14.454</td>\n",
       "      <td>0.115490</td>\n",
       "      <td>14605.0</td>\n",
       "      <td>36.992</td>\n",
       "      <td>-9.6391</td>\n",
       "      <td>64.2670</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99278</td>\n",
       "      <td>2.5891</td>\n",
       "      <td>430.400</td>\n",
       "      <td>-4.453500e+13</td>\n",
       "      <td>5.144900e+12</td>\n",
       "      <td>0.099591</td>\n",
       "      <td>6.5516</td>\n",
       "      <td>1887.5</td>\n",
       "      <td>43319.0</td>\n",
       "      <td>4.3931</td>\n",
       "      <td>0.260260</td>\n",
       "      <td>6.1052</td>\n",
       "      <td>1.013300e+11</td>\n",
       "      <td>357.2700</td>\n",
       "      <td>1476600.0</td>\n",
       "      <td>90.845</td>\n",
       "      <td>1.306200e+09</td>\n",
       "      <td>2.3731</td>\n",
       "      <td>391.3700</td>\n",
       "      <td>2965.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.49459</td>\n",
       "      <td>43.524</td>\n",
       "      <td>138.520</td>\n",
       "      <td>1.10790</td>\n",
       "      <td>0.91948</td>\n",
       "      <td>47.915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.510500e+12</td>\n",
       "      <td>3.4663</td>\n",
       "      <td>0.560950</td>\n",
       "      <td>4.13090</td>\n",
       "      <td>95531.0</td>\n",
       "      <td>39.486</td>\n",
       "      <td>-83.148</td>\n",
       "      <td>0.084881</td>\n",
       "      <td>0.032222</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>1.436500e+07</td>\n",
       "      <td>0.20102</td>\n",
       "      <td>-5.7688</td>\n",
       "      <td>1.2042</td>\n",
       "      <td>0.26290</td>\n",
       "      <td>8.13120</td>\n",
       "      <td>45119.00</td>\n",
       "      <td>1.1764</td>\n",
       "      <td>3.218100e+14</td>\n",
       "      <td>3838.2</td>\n",
       "      <td>0.40690</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.15236</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>780.100</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>0.51947</td>\n",
       "      <td>7.49140</td>\n",
       "      <td>112.51</td>\n",
       "      <td>259490.0</td>\n",
       "      <td>7.781400e+13</td>\n",
       "      <td>-36.837</td>\n",
       "      <td>1.10960</td>\n",
       "      <td>1.223100e+09</td>\n",
       "      <td>0.30944</td>\n",
       "      <td>10.370000</td>\n",
       "      <td>-0.106260</td>\n",
       "      <td>533.840</td>\n",
       "      <td>7.8490</td>\n",
       "      <td>1.0379</td>\n",
       "      <td>8.0030</td>\n",
       "      <td>12.349</td>\n",
       "      <td>-195.28</td>\n",
       "      <td>2.5598</td>\n",
       "      <td>92.1420</td>\n",
       "      <td>0.63789</td>\n",
       "      <td>1054.900</td>\n",
       "      <td>-1.204100e+10</td>\n",
       "      <td>5.187300e+12</td>\n",
       "      <td>1475400.0</td>\n",
       "      <td>1.036500</td>\n",
       "      <td>1.1903</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>301200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-9.299200e+13</td>\n",
       "      <td>-10.818</td>\n",
       "      <td>1020.30</td>\n",
       "      <td>2.9553</td>\n",
       "      <td>3342.5</td>\n",
       "      <td>-0.000372</td>\n",
       "      <td>17.0110</td>\n",
       "      <td>0.095268</td>\n",
       "      <td>5.7448</td>\n",
       "      <td>15.8830</td>\n",
       "      <td>0.037934</td>\n",
       "      <td>4.4860</td>\n",
       "      <td>-0.88909</td>\n",
       "      <td>8.4384</td>\n",
       "      <td>-1.189800</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>175.81</td>\n",
       "      <td>67.13300</td>\n",
       "      <td>119.26</td>\n",
       "      <td>0.007034</td>\n",
       "      <td>0.460040</td>\n",
       "      <td>-0.000705</td>\n",
       "      <td>-0.39149</td>\n",
       "      <td>2.0888</td>\n",
       "      <td>0.79790</td>\n",
       "      <td>0.135920</td>\n",
       "      <td>4.011100e+09</td>\n",
       "      <td>63.063</td>\n",
       "      <td>0.033075</td>\n",
       "      <td>75194.0</td>\n",
       "      <td>103.970</td>\n",
       "      <td>-15.4820</td>\n",
       "      <td>2.9432</td>\n",
       "      <td>1.1804</td>\n",
       "      <td>1.00700</td>\n",
       "      <td>2.1572</td>\n",
       "      <td>1251.500</td>\n",
       "      <td>1.894700e+15</td>\n",
       "      <td>1.077000e+10</td>\n",
       "      <td>0.992250</td>\n",
       "      <td>4.5331</td>\n",
       "      <td>14348.0</td>\n",
       "      <td>1575.7</td>\n",
       "      <td>9.8105</td>\n",
       "      <td>0.372830</td>\n",
       "      <td>1.5606</td>\n",
       "      <td>1.835400e+10</td>\n",
       "      <td>-3.4298</td>\n",
       "      <td>6485700.0</td>\n",
       "      <td>2120.000</td>\n",
       "      <td>3.081200e+10</td>\n",
       "      <td>34.0560</td>\n",
       "      <td>157.4300</td>\n",
       "      <td>3724.5</td>\n",
       "      <td>8.4211</td>\n",
       "      <td>0.40778</td>\n",
       "      <td>2971.200</td>\n",
       "      <td>204.700</td>\n",
       "      <td>-0.97998</td>\n",
       "      <td>9.94050</td>\n",
       "      <td>12011.000</td>\n",
       "      <td>0.99898</td>\n",
       "      <td>5.063400e+13</td>\n",
       "      <td>1.2261</td>\n",
       "      <td>0.250200</td>\n",
       "      <td>0.72974</td>\n",
       "      <td>373690.0</td>\n",
       "      <td>194.650</td>\n",
       "      <td>120.930</td>\n",
       "      <td>0.260710</td>\n",
       "      <td>0.234240</td>\n",
       "      <td>-0.002794</td>\n",
       "      <td>1.442300e+09</td>\n",
       "      <td>-0.01182</td>\n",
       "      <td>-34.8580</td>\n",
       "      <td>2.0694</td>\n",
       "      <td>0.79631</td>\n",
       "      <td>-16.33600</td>\n",
       "      <td>4952.40</td>\n",
       "      <td>1.1784</td>\n",
       "      <td>4.533000e+12</td>\n",
       "      <td>4889.1</td>\n",
       "      <td>0.51486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.11623</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>-109.150</td>\n",
       "      <td>0.297910</td>\n",
       "      <td>0.34490</td>\n",
       "      <td>-0.40932</td>\n",
       "      <td>2538.90</td>\n",
       "      <td>65332.0</td>\n",
       "      <td>1.907200e+15</td>\n",
       "      <td>144.120</td>\n",
       "      <td>1.05310</td>\n",
       "      <td>2.634100e+09</td>\n",
       "      <td>0.29782</td>\n",
       "      <td>2.654800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1808.900</td>\n",
       "      <td>7.2783</td>\n",
       "      <td>3.9757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29520.00</td>\n",
       "      <td>3.4225</td>\n",
       "      <td>96.7250</td>\n",
       "      <td>0.79725</td>\n",
       "      <td>215.570</td>\n",
       "      <td>1.732600e+13</td>\n",
       "      <td>2.635200e+12</td>\n",
       "      <td>2161200.0</td>\n",
       "      <td>0.895470</td>\n",
       "      <td>6.8257</td>\n",
       "      <td>0.97413</td>\n",
       "      <td>142620.0</td>\n",
       "      <td>231350.0</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>1.012500e+16</td>\n",
       "      <td>51.508</td>\n",
       "      <td>293.76</td>\n",
       "      <td>1.3351</td>\n",
       "      <td>3042.1</td>\n",
       "      <td>0.006791</td>\n",
       "      <td>94.8890</td>\n",
       "      <td>0.917090</td>\n",
       "      <td>8.7369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020281</td>\n",
       "      <td>3.9115</td>\n",
       "      <td>0.65634</td>\n",
       "      <td>6.1410</td>\n",
       "      <td>-1.089600</td>\n",
       "      <td>0.247940</td>\n",
       "      <td>7.9704</td>\n",
       "      <td>2063.10</td>\n",
       "      <td>0.80633</td>\n",
       "      <td>131.77</td>\n",
       "      <td>0.177960</td>\n",
       "      <td>0.989380</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>-0.98027</td>\n",
       "      <td>2.3610</td>\n",
       "      <td>0.58030</td>\n",
       "      <td>0.465770</td>\n",
       "      <td>5.702500e+09</td>\n",
       "      <td>23.738</td>\n",
       "      <td>-0.000847</td>\n",
       "      <td>75843.0</td>\n",
       "      <td>73.737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.5910</td>\n",
       "      <td>1.1029</td>\n",
       "      <td>0.98985</td>\n",
       "      <td>1.3446</td>\n",
       "      <td>519.200</td>\n",
       "      <td>5.693200e+14</td>\n",
       "      <td>2.869600e+11</td>\n",
       "      <td>0.011649</td>\n",
       "      <td>6.0236</td>\n",
       "      <td>1969.8</td>\n",
       "      <td>1967.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.085690</td>\n",
       "      <td>1.5846</td>\n",
       "      <td>3.825200e+10</td>\n",
       "      <td>130.7000</td>\n",
       "      <td>102100.0</td>\n",
       "      <td>1951.800</td>\n",
       "      <td>1.142800e+10</td>\n",
       "      <td>58.5660</td>\n",
       "      <td>176.8300</td>\n",
       "      <td>1279.0</td>\n",
       "      <td>4.9662</td>\n",
       "      <td>0.47912</td>\n",
       "      <td>-70.278</td>\n",
       "      <td>10.887</td>\n",
       "      <td>1.14340</td>\n",
       "      <td>6.19120</td>\n",
       "      <td>197.470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.574800e+13</td>\n",
       "      <td>1.0083</td>\n",
       "      <td>0.339530</td>\n",
       "      <td>13.48600</td>\n",
       "      <td>201300.0</td>\n",
       "      <td>38.842</td>\n",
       "      <td>324.000</td>\n",
       "      <td>0.238250</td>\n",
       "      <td>0.141550</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>5.830700e+09</td>\n",
       "      <td>0.92739</td>\n",
       "      <td>-13.6410</td>\n",
       "      <td>1.5298</td>\n",
       "      <td>1.14640</td>\n",
       "      <td>-0.43124</td>\n",
       "      <td>3856.50</td>\n",
       "      <td>1.4830</td>\n",
       "      <td>-8.991300e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       f1        f2         f3        f4       f5        f6       f7  \\\n",
       "0   0  0.10859  0.004314    -37.566  0.017364  0.28915 -10.25100   135.12   \n",
       "1   1  0.10090  0.299610  11822.000  0.276500  0.45970  -0.83733  1721.90   \n",
       "2   2  0.17803 -0.006980    907.270  0.272140  0.45948   0.17327  2298.00   \n",
       "3   3  0.15236  0.007259    780.100  0.025179  0.51947   7.49140   112.51   \n",
       "4   4  0.11623  0.502900   -109.150  0.297910  0.34490  -0.40932  2538.90   \n",
       "\n",
       "         f8            f9        f10      f11           f12      f13  \\\n",
       "0  168900.0  3.992400e+14     86.489  0.59881  1.423200e+09  0.27240   \n",
       "1  119810.0  3.874100e+15   9953.600  1.20930  3.334100e+09  0.28631   \n",
       "2  360650.0  1.224500e+13  15827.000  0.38164  1.230300e+09  0.25807   \n",
       "3  259490.0  7.781400e+13    -36.837  1.10960  1.223100e+09  0.30944   \n",
       "4   65332.0  1.907200e+15    144.120  1.05310  2.634100e+09  0.29782   \n",
       "\n",
       "         f14       f15       f16     f17     f18      f19      f20        f21  \\\n",
       "0   9.455600 -0.050305  1938.300  8.6331  4.0607  26.8670   -1.180   10961.00   \n",
       "1  -0.012858 -0.019912    10.284  6.1872  1.0419   4.6404   31.877  123620.00   \n",
       "2   2.455600       NaN    26.873  7.5463  1.9967   1.9526  817.760   -2948.70   \n",
       "3  10.370000 -0.106260   533.840  7.8490  1.0379   8.0030   12.349    -195.28   \n",
       "4   2.654800       NaN  1808.900  7.2783  3.9757      NaN      NaN   29520.00   \n",
       "\n",
       "      f22       f23      f24       f25           f26           f27        f28  \\\n",
       "0  1.5397  135.3200 -1.49650   440.080  2.590100e+12  2.194200e+09  2968800.0   \n",
       "1  1.3951  125.8100  1.19890   136.450  9.098100e+09  4.004100e+10  1564000.0   \n",
       "2  2.0054    1.6826  1.19680    74.624 -3.273900e+10  5.718900e+10    11058.0   \n",
       "3  2.5598   92.1420  0.63789  1054.900 -1.204100e+10  5.187300e+12  1475400.0   \n",
       "4  3.4225   96.7250  0.79725   215.570  1.732600e+13  2.635200e+12  2161200.0   \n",
       "\n",
       "        f29      f30      f31       f32       f33       f34           f35  \\\n",
       "0  0.001431  13.3270  0.75050   18509.0  146820.0 -0.000276  1.090600e+16   \n",
       "1  0.000204   3.1074  1.50330  238000.0   21440.0 -0.001344  3.079400e+16   \n",
       "2 -0.003097   8.0241  1.13180   27940.0  862460.0 -0.002207  5.849100e+13   \n",
       "3  1.036500   1.1903  0.98941  301200.0       NaN -0.000007 -9.299200e+13   \n",
       "4  0.895470   6.8257  0.97413  142620.0  231350.0  0.001257  1.012500e+16   \n",
       "\n",
       "        f36      f37     f38     f39       f40      f41       f42      f43  \\\n",
       "0  1705.400   414.29  3.5392  1888.0  0.968930  18.3880 -0.001583   7.7059   \n",
       "1   229.100   844.82  1.4680  4726.5  0.915380  -1.5321  0.982600   7.1112   \n",
       "2  -897.840      NaN  1.3561  3063.4  0.086232  16.1060  0.001481  11.4760   \n",
       "3   -10.818  1020.30  2.9553  3342.5 -0.000372  17.0110  0.095268   5.7448   \n",
       "4    51.508   293.76  1.3351  3042.1  0.006791  94.8890  0.917090   8.7369   \n",
       "\n",
       "       f44       f45     f46      f47      f48       f49       f50     f51  \\\n",
       "0   5.9325  0.025693  4.5604  0.61122  10.7950  0.341930  0.235010     NaN   \n",
       "1   2.0797  0.042321  4.2523  0.41871   5.4499  0.012737  0.386470  7.3082   \n",
       "2   5.3430  0.012162  4.1018 -0.88270   8.1228 -0.676690  0.337700 -1.0732   \n",
       "3  15.8830  0.037934  4.4860 -0.88909   8.4384 -1.189800  0.001391     NaN   \n",
       "4      NaN  0.020281  3.9115  0.65634   6.1410 -1.089600  0.247940  7.9704   \n",
       "\n",
       "       f52       f53     f54       f55       f56       f57      f58     f59  \\\n",
       "0  5237.70   1.29610  163.66  0.403780  0.188600 -0.001446 -0.35416  6.6432   \n",
       "1   283.21  -0.92552  140.80  0.247390 -0.001656 -0.000975 -0.22629  2.4246   \n",
       "2  4097.00  13.45800  159.24  0.322300  0.560090  0.000455 -0.16083  3.5753   \n",
       "3   175.81  67.13300  119.26  0.007034  0.460040 -0.000705 -0.39149  2.0888   \n",
       "4  2063.10   0.80633  131.77  0.177960  0.989380  0.000344 -0.98027  2.3610   \n",
       "\n",
       "       f60       f61           f62     f63       f64      f65      f66  \\\n",
       "0  0.30534  0.514020  1.907300e+09  29.861  0.965010   1797.2   72.178   \n",
       "1  0.77147  0.011613  1.803700e+09  64.604  0.262650   4455.0   78.339   \n",
       "2  0.60970  0.028301  5.271300e+08  14.454  0.115490  14605.0   36.992   \n",
       "3  0.79790  0.135920  4.011100e+09  63.063  0.033075  75194.0  103.970   \n",
       "4  0.58030  0.465770  5.702500e+09  23.738 -0.000847  75843.0   73.737   \n",
       "\n",
       "        f67      f68     f69      f70     f71       f72           f73  \\\n",
       "0  108.6200   1.9799  1.2907  0.99519  1.3228   827.340  7.779900e+14   \n",
       "1  745.5100   2.9069  1.4826  1.00510  1.4974    84.446  3.505600e+15   \n",
       "2   -9.6391  64.2670     NaN  0.99278  2.5891   430.400 -4.453500e+13   \n",
       "3  -15.4820   2.9432  1.1804  1.00700  2.1572  1251.500  1.894700e+15   \n",
       "4       NaN  64.5910  1.1029  0.98985  1.3446   519.200  5.693200e+14   \n",
       "\n",
       "            f74       f75     f76      f77      f78     f79       f80     f81  \\\n",
       "0  4.129900e+10  0.006994  6.9835  43956.0   1978.2  5.5084 -0.001081  6.1244   \n",
       "1  2.242300e+09  0.896300  4.6749  17713.0   9003.1 -4.3546  0.254100  6.9191   \n",
       "2  5.144900e+12  0.099591  6.5516   1887.5  43319.0  4.3931  0.260260  6.1052   \n",
       "3  1.077000e+10  0.992250  4.5331  14348.0   1575.7  9.8105  0.372830  1.5606   \n",
       "4  2.869600e+11  0.011649  6.0236   1969.8   1967.6     NaN  0.085690  1.5846   \n",
       "\n",
       "            f82       f83        f84       f85           f86      f87  \\\n",
       "0  1.231800e+11  275.9200  5308500.0  1704.000  5.022400e+10  53.3980   \n",
       "1  1.832400e+11    9.6510    32800.0  1480.600  2.300600e+10  44.0510   \n",
       "2  1.013300e+11  357.2700  1476600.0    90.845  1.306200e+09   2.3731   \n",
       "3  1.835400e+10   -3.4298  6485700.0  2120.000  3.081200e+10  34.0560   \n",
       "4  3.825200e+10  130.7000   102100.0  1951.800  1.142800e+10  58.5660   \n",
       "\n",
       "        f88     f89      f90      f91       f92      f93      f94       f95  \\\n",
       "0   -2.2012  6871.0   3.8862 -0.00558  5252.100  166.690  1.60740   0.66534   \n",
       "1  205.6900  4295.3  13.3880  0.46843   754.610   83.233  1.18900  29.55000   \n",
       "2  391.3700  2965.3      NaN  0.49459    43.524  138.520  1.10790   0.91948   \n",
       "3  157.4300  3724.5   8.4211  0.40778  2971.200  204.700 -0.97998   9.94050   \n",
       "4  176.8300  1279.0   4.9662  0.47912   -70.278   10.887  1.14340   6.19120   \n",
       "\n",
       "         f96      f97           f98     f99      f100      f101      f102  \\\n",
       "0   7768.900  0.99662  1.125700e+11  2.2432  0.934160   0.65056   94569.0   \n",
       "1   7343.700  0.99815  4.877700e+13  1.2708 -0.000969   5.29520    6779.0   \n",
       "2     47.915      NaN  1.510500e+12  3.4663  0.560950   4.13090   95531.0   \n",
       "3  12011.000  0.99898  5.063400e+13  1.2261  0.250200   0.72974  373690.0   \n",
       "4    197.470      NaN  1.574800e+13  1.0083  0.339530  13.48600  201300.0   \n",
       "\n",
       "      f103      f104      f105      f106      f107          f108     f109  \\\n",
       "0   21.471  8214.100  0.288010  0.097826  0.001071  1.412400e+09  0.11093   \n",
       "1  227.720    34.342  0.340300  0.143370  0.049276  1.903200e+09  0.97673   \n",
       "2   39.486   -83.148  0.084881  0.032222  0.001668  1.436500e+07  0.20102   \n",
       "3  194.650   120.930  0.260710  0.234240 -0.002794  1.442300e+09 -0.01182   \n",
       "4   38.842   324.000  0.238250  0.141550  0.002208  5.830700e+09  0.92739   \n",
       "\n",
       "      f110    f111     f112      f113      f114    f115          f116    f117  \\\n",
       "0 -12.2280  1.7482  1.90960  -7.11570   4378.80  1.2096  8.613400e+14   140.1   \n",
       "1 -56.7580  4.1684  0.34808   4.14200    913.23  1.2464  7.575100e+15  1861.0   \n",
       "2  -5.7688  1.2042  0.26290   8.13120  45119.00  1.1764  3.218100e+14  3838.2   \n",
       "3 -34.8580  2.0694  0.79631 -16.33600   4952.40  1.1784  4.533000e+12  4889.1   \n",
       "4 -13.6410  1.5298  1.14640  -0.43124   3856.50  1.4830 -8.991300e+12     NaN   \n",
       "\n",
       "      f118  claim  \n",
       "0  1.01770      1  \n",
       "1  0.28359      0  \n",
       "2  0.40690      1  \n",
       "3  0.51486      1  \n",
       "4  0.23049      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 999\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in train.columns if 'f' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling and imputing of missing values\n",
    "sc = StandardScaler()\n",
    "si = SimpleImputer()\n",
    "\n",
    "train[feature_cols] = si.fit_transform(sc.fit_transform(train[feature_cols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc Z scores for df\n",
    "z_scores = stats.zscore(train[feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the number of outliers by column\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = np.count_nonzero(abs_z_scores>3,axis=1)\n",
    "train['outlier'] = outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop any records with more than 6 outlier columns\n",
    "X = train[train['outlier']<6][feature_cols]\n",
    "y = train[train['outlier']<6]['claim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape  765483\n",
      "y train shape  765483\n",
      "X test shape  191371\n",
      "y test shape  191371\n"
     ]
    }
   ],
   "source": [
    "print('X train shape ',len(X_train))\n",
    "print('y train shape ',len(y_train))\n",
    "print('X test shape ',len(X_test))\n",
    "print('y test shape ',len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set Stratified K Fold\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup XGBoost model (need GPU to run tree_method = 'gpu hist')\n",
    "model_xgb =  XGBClassifier(max_depth=3,\n",
    "                              colsample_bytree=0.7,\n",
    "                              n_estimators=20000,\n",
    "                              learning_rate=0.02,\n",
    "                              objective='binary:logistic', \n",
    "                              verbosity =1,\n",
    "                              eval_metric  = 'auc',\n",
    "                              tree_method='gpu_hist',\n",
    "                              n_jobs=-1,\n",
    "                              use_label_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMClassifier(objective= 'binary',\n",
    "    n_estimators= 20000,\n",
    "    random_state= 42,\n",
    "    learning_rate= 0.02,\n",
    "    subsample= 0.6,\n",
    "    subsample_freq= 1,\n",
    "    colsample_bytree= 0.4,\n",
    "    reg_alpha= 10.0,\n",
    "    reg_lambda= 1e-1,\n",
    "    min_child_weight= 256,\n",
    "    min_child_samples= 20,\n",
    "    max_depth = 3,\n",
    "    num_leaves = 7,\n",
    "    eval_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 started...\n",
      "[0]\tvalidation_0-auc:0.52116\n",
      "[200]\tvalidation_0-auc:0.67179\n",
      "[400]\tvalidation_0-auc:0.71429\n",
      "[600]\tvalidation_0-auc:0.73608\n",
      "[800]\tvalidation_0-auc:0.75015\n",
      "[1000]\tvalidation_0-auc:0.76123\n",
      "[1200]\tvalidation_0-auc:0.76946\n",
      "[1400]\tvalidation_0-auc:0.77559\n",
      "[1600]\tvalidation_0-auc:0.78062\n",
      "[1800]\tvalidation_0-auc:0.78480\n",
      "[2000]\tvalidation_0-auc:0.78797\n",
      "[2200]\tvalidation_0-auc:0.79013\n",
      "[2400]\tvalidation_0-auc:0.79174\n",
      "[2600]\tvalidation_0-auc:0.79292\n",
      "[2800]\tvalidation_0-auc:0.79390\n",
      "[3000]\tvalidation_0-auc:0.79471\n",
      "[3200]\tvalidation_0-auc:0.79544\n",
      "[3400]\tvalidation_0-auc:0.79594\n",
      "[3600]\tvalidation_0-auc:0.79632\n",
      "[3800]\tvalidation_0-auc:0.79668\n",
      "[4000]\tvalidation_0-auc:0.79703\n",
      "[4200]\tvalidation_0-auc:0.79733\n",
      "[4400]\tvalidation_0-auc:0.79755\n",
      "[4600]\tvalidation_0-auc:0.79782\n",
      "[4800]\tvalidation_0-auc:0.79799\n",
      "[5000]\tvalidation_0-auc:0.79819\n",
      "[5200]\tvalidation_0-auc:0.79834\n",
      "[5400]\tvalidation_0-auc:0.79849\n",
      "[5600]\tvalidation_0-auc:0.79865\n",
      "[5800]\tvalidation_0-auc:0.79877\n",
      "[6000]\tvalidation_0-auc:0.79887\n",
      "[6200]\tvalidation_0-auc:0.79899\n",
      "[6400]\tvalidation_0-auc:0.79908\n",
      "[6600]\tvalidation_0-auc:0.79917\n",
      "[6800]\tvalidation_0-auc:0.79924\n",
      "[7000]\tvalidation_0-auc:0.79933\n",
      "[7200]\tvalidation_0-auc:0.79939\n",
      "[7400]\tvalidation_0-auc:0.79945\n",
      "[7600]\tvalidation_0-auc:0.79951\n",
      "[7800]\tvalidation_0-auc:0.79956\n",
      "[8000]\tvalidation_0-auc:0.79964\n",
      "[8200]\tvalidation_0-auc:0.79970\n",
      "[8400]\tvalidation_0-auc:0.79971\n",
      "[8600]\tvalidation_0-auc:0.79976\n",
      "[8800]\tvalidation_0-auc:0.79982\n",
      "[9000]\tvalidation_0-auc:0.79983\n",
      "[9200]\tvalidation_0-auc:0.79987\n",
      "[9400]\tvalidation_0-auc:0.79991\n",
      "[9600]\tvalidation_0-auc:0.79990\n",
      "[9610]\tvalidation_0-auc:0.79990\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.672022\tvalid_0's binary_logloss: 0.679481\n",
      "[400]\tvalid_0's auc: 0.713666\tvalid_0's binary_logloss: 0.666912\n",
      "[600]\tvalid_0's auc: 0.737326\tvalid_0's binary_logloss: 0.655988\n",
      "[800]\tvalid_0's auc: 0.751008\tvalid_0's binary_logloss: 0.64693\n",
      "[1000]\tvalid_0's auc: 0.761991\tvalid_0's binary_logloss: 0.638722\n",
      "[1200]\tvalid_0's auc: 0.770096\tvalid_0's binary_logloss: 0.631395\n",
      "[1400]\tvalid_0's auc: 0.775674\tvalid_0's binary_logloss: 0.625234\n",
      "[1600]\tvalid_0's auc: 0.780547\tvalid_0's binary_logloss: 0.619597\n",
      "[1800]\tvalid_0's auc: 0.783942\tvalid_0's binary_logloss: 0.614906\n",
      "[2000]\tvalid_0's auc: 0.786475\tvalid_0's binary_logloss: 0.610572\n",
      "[2200]\tvalid_0's auc: 0.788465\tvalid_0's binary_logloss: 0.606998\n",
      "[2400]\tvalid_0's auc: 0.79005\tvalid_0's binary_logloss: 0.60369\n",
      "[2600]\tvalid_0's auc: 0.79153\tvalid_0's binary_logloss: 0.600506\n",
      "[2800]\tvalid_0's auc: 0.792611\tvalid_0's binary_logloss: 0.597741\n",
      "[3000]\tvalid_0's auc: 0.793779\tvalid_0's binary_logloss: 0.595053\n",
      "[3200]\tvalid_0's auc: 0.794334\tvalid_0's binary_logloss: 0.593125\n",
      "[3400]\tvalid_0's auc: 0.794958\tvalid_0's binary_logloss: 0.591159\n",
      "[3600]\tvalid_0's auc: 0.795519\tvalid_0's binary_logloss: 0.589461\n",
      "[3800]\tvalid_0's auc: 0.795847\tvalid_0's binary_logloss: 0.588074\n",
      "[4000]\tvalid_0's auc: 0.796292\tvalid_0's binary_logloss: 0.586627\n",
      "[4200]\tvalid_0's auc: 0.796684\tvalid_0's binary_logloss: 0.58526\n",
      "[4400]\tvalid_0's auc: 0.797013\tvalid_0's binary_logloss: 0.584061\n",
      "[4600]\tvalid_0's auc: 0.797309\tvalid_0's binary_logloss: 0.583005\n",
      "[4800]\tvalid_0's auc: 0.797545\tvalid_0's binary_logloss: 0.582079\n",
      "[5000]\tvalid_0's auc: 0.797835\tvalid_0's binary_logloss: 0.581113\n",
      "[5200]\tvalid_0's auc: 0.79809\tvalid_0's binary_logloss: 0.580212\n",
      "[5400]\tvalid_0's auc: 0.798257\tvalid_0's binary_logloss: 0.579509\n",
      "[5600]\tvalid_0's auc: 0.798397\tvalid_0's binary_logloss: 0.578867\n",
      "[5800]\tvalid_0's auc: 0.798575\tvalid_0's binary_logloss: 0.578226\n",
      "[6000]\tvalid_0's auc: 0.798702\tvalid_0's binary_logloss: 0.577676\n",
      "[6200]\tvalid_0's auc: 0.798819\tvalid_0's binary_logloss: 0.577153\n",
      "[6400]\tvalid_0's auc: 0.798973\tvalid_0's binary_logloss: 0.57666\n",
      "[6600]\tvalid_0's auc: 0.799059\tvalid_0's binary_logloss: 0.576242\n",
      "[6800]\tvalid_0's auc: 0.799175\tvalid_0's binary_logloss: 0.575799\n",
      "[7000]\tvalid_0's auc: 0.799253\tvalid_0's binary_logloss: 0.575444\n",
      "[7200]\tvalid_0's auc: 0.799345\tvalid_0's binary_logloss: 0.575074\n",
      "[7400]\tvalid_0's auc: 0.79943\tvalid_0's binary_logloss: 0.574778\n",
      "[7600]\tvalid_0's auc: 0.799479\tvalid_0's binary_logloss: 0.574499\n",
      "[7800]\tvalid_0's auc: 0.799555\tvalid_0's binary_logloss: 0.574223\n",
      "[8000]\tvalid_0's auc: 0.799622\tvalid_0's binary_logloss: 0.573971\n",
      "[8200]\tvalid_0's auc: 0.799657\tvalid_0's binary_logloss: 0.573753\n",
      "[8400]\tvalid_0's auc: 0.799709\tvalid_0's binary_logloss: 0.57353\n",
      "[8600]\tvalid_0's auc: 0.799733\tvalid_0's binary_logloss: 0.57334\n",
      "[8800]\tvalid_0's auc: 0.79975\tvalid_0's binary_logloss: 0.573172\n",
      "[9000]\tvalid_0's auc: 0.799795\tvalid_0's binary_logloss: 0.572984\n",
      "[9200]\tvalid_0's auc: 0.799837\tvalid_0's binary_logloss: 0.572806\n",
      "[9400]\tvalid_0's auc: 0.799885\tvalid_0's binary_logloss: 0.572661\n",
      "[9600]\tvalid_0's auc: 0.799907\tvalid_0's binary_logloss: 0.572517\n",
      "[9800]\tvalid_0's auc: 0.799935\tvalid_0's binary_logloss: 0.572364\n",
      "[10000]\tvalid_0's auc: 0.79994\tvalid_0's binary_logloss: 0.57225\n",
      "[10200]\tvalid_0's auc: 0.79996\tvalid_0's binary_logloss: 0.572132\n",
      "[10400]\tvalid_0's auc: 0.800001\tvalid_0's binary_logloss: 0.572008\n",
      "[10600]\tvalid_0's auc: 0.80004\tvalid_0's binary_logloss: 0.571896\n",
      "[10800]\tvalid_0's auc: 0.800075\tvalid_0's binary_logloss: 0.571786\n",
      "[11000]\tvalid_0's auc: 0.800088\tvalid_0's binary_logloss: 0.571685\n",
      "Early stopping, best iteration is:\n",
      "[10867]\tvalid_0's auc: 0.800096\tvalid_0's binary_logloss: 0.571738\n",
      "Batch 1 started...\n",
      "[0]\tvalidation_0-auc:0.51758\n",
      "[200]\tvalidation_0-auc:0.66359\n",
      "[400]\tvalidation_0-auc:0.71215\n",
      "[600]\tvalidation_0-auc:0.73374\n",
      "[800]\tvalidation_0-auc:0.74800\n",
      "[1000]\tvalidation_0-auc:0.75765\n",
      "[1200]\tvalidation_0-auc:0.76532\n",
      "[1400]\tvalidation_0-auc:0.77081\n",
      "[1600]\tvalidation_0-auc:0.77519\n",
      "[1800]\tvalidation_0-auc:0.77924\n",
      "[2000]\tvalidation_0-auc:0.78245\n",
      "[2200]\tvalidation_0-auc:0.78457\n",
      "[2400]\tvalidation_0-auc:0.78627\n",
      "[2600]\tvalidation_0-auc:0.78751\n",
      "[2800]\tvalidation_0-auc:0.78837\n",
      "[3000]\tvalidation_0-auc:0.78917\n",
      "[3200]\tvalidation_0-auc:0.78988\n",
      "[3400]\tvalidation_0-auc:0.79046\n",
      "[3600]\tvalidation_0-auc:0.79089\n",
      "[3800]\tvalidation_0-auc:0.79125\n",
      "[4000]\tvalidation_0-auc:0.79157\n",
      "[4200]\tvalidation_0-auc:0.79190\n",
      "[4400]\tvalidation_0-auc:0.79221\n",
      "[4600]\tvalidation_0-auc:0.79244\n",
      "[4800]\tvalidation_0-auc:0.79266\n",
      "[5000]\tvalidation_0-auc:0.79283\n",
      "[5200]\tvalidation_0-auc:0.79298\n",
      "[5400]\tvalidation_0-auc:0.79314\n",
      "[5600]\tvalidation_0-auc:0.79329\n",
      "[5800]\tvalidation_0-auc:0.79343\n",
      "[6000]\tvalidation_0-auc:0.79357\n",
      "[6200]\tvalidation_0-auc:0.79368\n",
      "[6400]\tvalidation_0-auc:0.79378\n",
      "[6600]\tvalidation_0-auc:0.79387\n",
      "[6800]\tvalidation_0-auc:0.79397\n",
      "[7000]\tvalidation_0-auc:0.79402\n",
      "[7200]\tvalidation_0-auc:0.79409\n",
      "[7400]\tvalidation_0-auc:0.79414\n",
      "[7600]\tvalidation_0-auc:0.79419\n",
      "[7800]\tvalidation_0-auc:0.79423\n",
      "[8000]\tvalidation_0-auc:0.79424\n",
      "[8200]\tvalidation_0-auc:0.79429\n",
      "[8400]\tvalidation_0-auc:0.79434\n",
      "[8600]\tvalidation_0-auc:0.79435\n",
      "[8800]\tvalidation_0-auc:0.79441\n",
      "[9000]\tvalidation_0-auc:0.79441\n",
      "[9200]\tvalidation_0-auc:0.79443\n",
      "[9373]\tvalidation_0-auc:0.79443\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.666837\tvalid_0's binary_logloss: 0.679373\n",
      "[400]\tvalid_0's auc: 0.712866\tvalid_0's binary_logloss: 0.666921\n",
      "[600]\tvalid_0's auc: 0.734642\tvalid_0's binary_logloss: 0.656261\n",
      "[800]\tvalid_0's auc: 0.747283\tvalid_0's binary_logloss: 0.647724\n",
      "[1000]\tvalid_0's auc: 0.75706\tvalid_0's binary_logloss: 0.639969\n",
      "[1200]\tvalid_0's auc: 0.765129\tvalid_0's binary_logloss: 0.63305\n",
      "[1400]\tvalid_0's auc: 0.771692\tvalid_0's binary_logloss: 0.626772\n",
      "[1600]\tvalid_0's auc: 0.776477\tvalid_0's binary_logloss: 0.62121\n",
      "[1800]\tvalid_0's auc: 0.780211\tvalid_0's binary_logloss: 0.616256\n",
      "[2000]\tvalid_0's auc: 0.78273\tvalid_0's binary_logloss: 0.611963\n",
      "[2200]\tvalid_0's auc: 0.784522\tvalid_0's binary_logloss: 0.608417\n",
      "[2400]\tvalid_0's auc: 0.786109\tvalid_0's binary_logloss: 0.605125\n",
      "[2600]\tvalid_0's auc: 0.787112\tvalid_0's binary_logloss: 0.602423\n",
      "[2800]\tvalid_0's auc: 0.788168\tvalid_0's binary_logloss: 0.599854\n",
      "[3000]\tvalid_0's auc: 0.789052\tvalid_0's binary_logloss: 0.597494\n",
      "[3200]\tvalid_0's auc: 0.789735\tvalid_0's binary_logloss: 0.595481\n",
      "[3400]\tvalid_0's auc: 0.790403\tvalid_0's binary_logloss: 0.593523\n",
      "[3600]\tvalid_0's auc: 0.790974\tvalid_0's binary_logloss: 0.591785\n",
      "[3800]\tvalid_0's auc: 0.791434\tvalid_0's binary_logloss: 0.590267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000]\tvalid_0's auc: 0.791748\tvalid_0's binary_logloss: 0.589047\n",
      "[4200]\tvalid_0's auc: 0.792114\tvalid_0's binary_logloss: 0.587813\n",
      "[4400]\tvalid_0's auc: 0.792374\tvalid_0's binary_logloss: 0.586784\n",
      "[4600]\tvalid_0's auc: 0.792649\tvalid_0's binary_logloss: 0.585782\n",
      "[4800]\tvalid_0's auc: 0.792885\tvalid_0's binary_logloss: 0.584865\n",
      "[5000]\tvalid_0's auc: 0.79308\tvalid_0's binary_logloss: 0.584115\n",
      "[5200]\tvalid_0's auc: 0.793304\tvalid_0's binary_logloss: 0.583335\n",
      "[5400]\tvalid_0's auc: 0.793466\tvalid_0's binary_logloss: 0.582642\n",
      "[5600]\tvalid_0's auc: 0.793651\tvalid_0's binary_logloss: 0.58201\n",
      "[5800]\tvalid_0's auc: 0.79381\tvalid_0's binary_logloss: 0.581454\n",
      "[6000]\tvalid_0's auc: 0.793926\tvalid_0's binary_logloss: 0.580956\n",
      "[6200]\tvalid_0's auc: 0.79403\tvalid_0's binary_logloss: 0.580508\n",
      "[6400]\tvalid_0's auc: 0.794097\tvalid_0's binary_logloss: 0.580104\n",
      "[6600]\tvalid_0's auc: 0.794175\tvalid_0's binary_logloss: 0.579714\n",
      "[6800]\tvalid_0's auc: 0.794247\tvalid_0's binary_logloss: 0.579363\n",
      "[7000]\tvalid_0's auc: 0.794316\tvalid_0's binary_logloss: 0.578997\n",
      "[7200]\tvalid_0's auc: 0.794393\tvalid_0's binary_logloss: 0.578687\n",
      "[7400]\tvalid_0's auc: 0.79446\tvalid_0's binary_logloss: 0.578405\n",
      "[7600]\tvalid_0's auc: 0.794514\tvalid_0's binary_logloss: 0.578144\n",
      "[7800]\tvalid_0's auc: 0.79455\tvalid_0's binary_logloss: 0.577915\n",
      "[8000]\tvalid_0's auc: 0.794622\tvalid_0's binary_logloss: 0.577678\n",
      "[8200]\tvalid_0's auc: 0.794657\tvalid_0's binary_logloss: 0.577483\n",
      "[8400]\tvalid_0's auc: 0.794736\tvalid_0's binary_logloss: 0.577286\n",
      "[8600]\tvalid_0's auc: 0.794812\tvalid_0's binary_logloss: 0.577073\n",
      "[8800]\tvalid_0's auc: 0.794862\tvalid_0's binary_logloss: 0.576904\n",
      "[9000]\tvalid_0's auc: 0.794908\tvalid_0's binary_logloss: 0.576746\n",
      "[9200]\tvalid_0's auc: 0.794975\tvalid_0's binary_logloss: 0.576585\n",
      "[9400]\tvalid_0's auc: 0.795007\tvalid_0's binary_logloss: 0.576443\n",
      "[9600]\tvalid_0's auc: 0.795054\tvalid_0's binary_logloss: 0.576309\n",
      "[9800]\tvalid_0's auc: 0.795109\tvalid_0's binary_logloss: 0.576181\n",
      "[10000]\tvalid_0's auc: 0.795139\tvalid_0's binary_logloss: 0.576072\n",
      "[10200]\tvalid_0's auc: 0.795175\tvalid_0's binary_logloss: 0.575968\n",
      "[10400]\tvalid_0's auc: 0.79521\tvalid_0's binary_logloss: 0.57588\n",
      "[10600]\tvalid_0's auc: 0.795234\tvalid_0's binary_logloss: 0.575785\n",
      "[10800]\tvalid_0's auc: 0.795256\tvalid_0's binary_logloss: 0.575696\n",
      "[11000]\tvalid_0's auc: 0.795276\tvalid_0's binary_logloss: 0.575618\n",
      "Early stopping, best iteration is:\n",
      "[10983]\tvalid_0's auc: 0.795283\tvalid_0's binary_logloss: 0.575621\n",
      "Batch 2 started...\n",
      "[0]\tvalidation_0-auc:0.51891\n",
      "[200]\tvalidation_0-auc:0.67268\n",
      "[400]\tvalidation_0-auc:0.71412\n",
      "[600]\tvalidation_0-auc:0.73625\n",
      "[800]\tvalidation_0-auc:0.75037\n",
      "[1000]\tvalidation_0-auc:0.76120\n",
      "[1200]\tvalidation_0-auc:0.76841\n",
      "[1400]\tvalidation_0-auc:0.77457\n",
      "[1600]\tvalidation_0-auc:0.77926\n",
      "[1800]\tvalidation_0-auc:0.78340\n",
      "[2000]\tvalidation_0-auc:0.78636\n",
      "[2200]\tvalidation_0-auc:0.78843\n",
      "[2400]\tvalidation_0-auc:0.78988\n",
      "[2600]\tvalidation_0-auc:0.79120\n",
      "[2800]\tvalidation_0-auc:0.79219\n",
      "[3000]\tvalidation_0-auc:0.79295\n",
      "[3200]\tvalidation_0-auc:0.79362\n",
      "[3400]\tvalidation_0-auc:0.79416\n",
      "[3600]\tvalidation_0-auc:0.79459\n",
      "[3800]\tvalidation_0-auc:0.79499\n",
      "[4000]\tvalidation_0-auc:0.79525\n",
      "[4200]\tvalidation_0-auc:0.79551\n",
      "[4400]\tvalidation_0-auc:0.79578\n",
      "[4600]\tvalidation_0-auc:0.79596\n",
      "[4800]\tvalidation_0-auc:0.79614\n",
      "[5000]\tvalidation_0-auc:0.79632\n",
      "[5200]\tvalidation_0-auc:0.79648\n",
      "[5400]\tvalidation_0-auc:0.79661\n",
      "[5600]\tvalidation_0-auc:0.79674\n",
      "[5800]\tvalidation_0-auc:0.79689\n",
      "[6000]\tvalidation_0-auc:0.79698\n",
      "[6200]\tvalidation_0-auc:0.79703\n",
      "[6400]\tvalidation_0-auc:0.79709\n",
      "[6600]\tvalidation_0-auc:0.79716\n",
      "[6800]\tvalidation_0-auc:0.79722\n",
      "[7000]\tvalidation_0-auc:0.79725\n",
      "[7200]\tvalidation_0-auc:0.79728\n",
      "[7400]\tvalidation_0-auc:0.79732\n",
      "[7600]\tvalidation_0-auc:0.79742\n",
      "[7800]\tvalidation_0-auc:0.79746\n",
      "[8000]\tvalidation_0-auc:0.79750\n",
      "[8200]\tvalidation_0-auc:0.79751\n",
      "[8400]\tvalidation_0-auc:0.79754\n",
      "[8600]\tvalidation_0-auc:0.79756\n",
      "[8800]\tvalidation_0-auc:0.79757\n",
      "[9000]\tvalidation_0-auc:0.79761\n",
      "[9200]\tvalidation_0-auc:0.79765\n",
      "[9400]\tvalidation_0-auc:0.79766\n",
      "[9600]\tvalidation_0-auc:0.79770\n",
      "[9800]\tvalidation_0-auc:0.79772\n",
      "[10000]\tvalidation_0-auc:0.79774\n",
      "[10200]\tvalidation_0-auc:0.79775\n",
      "[10400]\tvalidation_0-auc:0.79776\n",
      "[10600]\tvalidation_0-auc:0.79777\n",
      "[10800]\tvalidation_0-auc:0.79779\n",
      "[11000]\tvalidation_0-auc:0.79779\n",
      "[11200]\tvalidation_0-auc:0.79779\n",
      "[11241]\tvalidation_0-auc:0.79779\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.675177\tvalid_0's binary_logloss: 0.679153\n",
      "[400]\tvalid_0's auc: 0.714339\tvalid_0's binary_logloss: 0.666843\n",
      "[600]\tvalid_0's auc: 0.739067\tvalid_0's binary_logloss: 0.655939\n",
      "[800]\tvalid_0's auc: 0.752093\tvalid_0's binary_logloss: 0.646756\n",
      "[1000]\tvalid_0's auc: 0.761468\tvalid_0's binary_logloss: 0.638861\n",
      "[1200]\tvalid_0's auc: 0.76841\tvalid_0's binary_logloss: 0.631805\n",
      "[1400]\tvalid_0's auc: 0.774265\tvalid_0's binary_logloss: 0.625881\n",
      "[1600]\tvalid_0's auc: 0.779524\tvalid_0's binary_logloss: 0.620128\n",
      "[1800]\tvalid_0's auc: 0.783289\tvalid_0's binary_logloss: 0.615057\n",
      "[2000]\tvalid_0's auc: 0.785576\tvalid_0's binary_logloss: 0.610996\n",
      "[2200]\tvalid_0's auc: 0.787816\tvalid_0's binary_logloss: 0.607058\n",
      "[2400]\tvalid_0's auc: 0.789246\tvalid_0's binary_logloss: 0.603719\n",
      "[2600]\tvalid_0's auc: 0.790262\tvalid_0's binary_logloss: 0.600899\n",
      "[2800]\tvalid_0's auc: 0.791408\tvalid_0's binary_logloss: 0.598105\n",
      "[3000]\tvalid_0's auc: 0.792146\tvalid_0's binary_logloss: 0.595936\n",
      "[3200]\tvalid_0's auc: 0.792907\tvalid_0's binary_logloss: 0.593774\n",
      "[3400]\tvalid_0's auc: 0.793525\tvalid_0's binary_logloss: 0.591842\n",
      "[3600]\tvalid_0's auc: 0.793909\tvalid_0's binary_logloss: 0.59029\n",
      "[3800]\tvalid_0's auc: 0.794406\tvalid_0's binary_logloss: 0.588732\n",
      "[4000]\tvalid_0's auc: 0.794762\tvalid_0's binary_logloss: 0.587354\n",
      "[4200]\tvalid_0's auc: 0.795093\tvalid_0's binary_logloss: 0.586124\n",
      "[4400]\tvalid_0's auc: 0.795383\tvalid_0's binary_logloss: 0.585006\n",
      "[4600]\tvalid_0's auc: 0.795656\tvalid_0's binary_logloss: 0.584003\n",
      "[4800]\tvalid_0's auc: 0.795959\tvalid_0's binary_logloss: 0.582975\n",
      "[5000]\tvalid_0's auc: 0.796226\tvalid_0's binary_logloss: 0.582076\n",
      "[5200]\tvalid_0's auc: 0.796382\tvalid_0's binary_logloss: 0.581332\n",
      "[5400]\tvalid_0's auc: 0.796596\tvalid_0's binary_logloss: 0.58057\n",
      "[5600]\tvalid_0's auc: 0.796724\tvalid_0's binary_logloss: 0.579968\n",
      "[5800]\tvalid_0's auc: 0.796848\tvalid_0's binary_logloss: 0.579426\n",
      "[6000]\tvalid_0's auc: 0.796928\tvalid_0's binary_logloss: 0.578932\n",
      "[6200]\tvalid_0's auc: 0.797073\tvalid_0's binary_logloss: 0.578432\n",
      "[6400]\tvalid_0's auc: 0.797203\tvalid_0's binary_logloss: 0.577965\n",
      "[6600]\tvalid_0's auc: 0.797284\tvalid_0's binary_logloss: 0.577565\n",
      "[6800]\tvalid_0's auc: 0.797357\tvalid_0's binary_logloss: 0.577189\n",
      "[7000]\tvalid_0's auc: 0.79745\tvalid_0's binary_logloss: 0.576823\n",
      "[7200]\tvalid_0's auc: 0.797509\tvalid_0's binary_logloss: 0.576502\n",
      "[7400]\tvalid_0's auc: 0.79754\tvalid_0's binary_logloss: 0.576253\n",
      "[7600]\tvalid_0's auc: 0.797614\tvalid_0's binary_logloss: 0.575993\n",
      "[7800]\tvalid_0's auc: 0.797633\tvalid_0's binary_logloss: 0.575757\n",
      "[8000]\tvalid_0's auc: 0.797671\tvalid_0's binary_logloss: 0.575544\n",
      "[8200]\tvalid_0's auc: 0.79775\tvalid_0's binary_logloss: 0.575292\n",
      "[8400]\tvalid_0's auc: 0.797757\tvalid_0's binary_logloss: 0.575118\n",
      "[8600]\tvalid_0's auc: 0.797805\tvalid_0's binary_logloss: 0.574933\n",
      "[8800]\tvalid_0's auc: 0.797837\tvalid_0's binary_logloss: 0.574737\n",
      "[9000]\tvalid_0's auc: 0.797881\tvalid_0's binary_logloss: 0.574557\n",
      "[9200]\tvalid_0's auc: 0.797931\tvalid_0's binary_logloss: 0.574376\n",
      "[9400]\tvalid_0's auc: 0.797954\tvalid_0's binary_logloss: 0.574247\n",
      "[9600]\tvalid_0's auc: 0.79803\tvalid_0's binary_logloss: 0.574097\n",
      "[9800]\tvalid_0's auc: 0.79806\tvalid_0's binary_logloss: 0.573963\n",
      "[10000]\tvalid_0's auc: 0.798073\tvalid_0's binary_logloss: 0.573857\n",
      "Early stopping, best iteration is:\n",
      "[9947]\tvalid_0's auc: 0.798101\tvalid_0's binary_logloss: 0.573866\n",
      "Batch 3 started...\n",
      "[0]\tvalidation_0-auc:0.52093\n",
      "[200]\tvalidation_0-auc:0.67246\n",
      "[400]\tvalidation_0-auc:0.71785\n",
      "[600]\tvalidation_0-auc:0.73997\n",
      "[800]\tvalidation_0-auc:0.75239\n",
      "[1000]\tvalidation_0-auc:0.76269\n",
      "[1200]\tvalidation_0-auc:0.77052\n",
      "[1400]\tvalidation_0-auc:0.77649\n",
      "[1600]\tvalidation_0-auc:0.78135\n",
      "[1800]\tvalidation_0-auc:0.78509\n",
      "[2000]\tvalidation_0-auc:0.78798\n",
      "[2200]\tvalidation_0-auc:0.79001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2400]\tvalidation_0-auc:0.79143\n",
      "[2600]\tvalidation_0-auc:0.79261\n",
      "[2800]\tvalidation_0-auc:0.79355\n",
      "[3000]\tvalidation_0-auc:0.79430\n",
      "[3200]\tvalidation_0-auc:0.79486\n",
      "[3400]\tvalidation_0-auc:0.79534\n",
      "[3600]\tvalidation_0-auc:0.79583\n",
      "[3800]\tvalidation_0-auc:0.79611\n",
      "[4000]\tvalidation_0-auc:0.79634\n",
      "[4200]\tvalidation_0-auc:0.79651\n",
      "[4400]\tvalidation_0-auc:0.79675\n",
      "[4600]\tvalidation_0-auc:0.79701\n",
      "[4800]\tvalidation_0-auc:0.79718\n",
      "[5000]\tvalidation_0-auc:0.79734\n",
      "[5200]\tvalidation_0-auc:0.79749\n",
      "[5400]\tvalidation_0-auc:0.79757\n",
      "[5600]\tvalidation_0-auc:0.79765\n",
      "[5800]\tvalidation_0-auc:0.79771\n",
      "[6000]\tvalidation_0-auc:0.79782\n",
      "[6200]\tvalidation_0-auc:0.79790\n",
      "[6400]\tvalidation_0-auc:0.79799\n",
      "[6600]\tvalidation_0-auc:0.79803\n",
      "[6800]\tvalidation_0-auc:0.79804\n",
      "[7000]\tvalidation_0-auc:0.79811\n",
      "[7200]\tvalidation_0-auc:0.79817\n",
      "[7400]\tvalidation_0-auc:0.79824\n",
      "[7600]\tvalidation_0-auc:0.79829\n",
      "[7800]\tvalidation_0-auc:0.79833\n",
      "[8000]\tvalidation_0-auc:0.79835\n",
      "[8200]\tvalidation_0-auc:0.79838\n",
      "[8400]\tvalidation_0-auc:0.79841\n",
      "[8600]\tvalidation_0-auc:0.79843\n",
      "[8800]\tvalidation_0-auc:0.79844\n",
      "[9000]\tvalidation_0-auc:0.79844\n",
      "[9019]\tvalidation_0-auc:0.79844\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.676438\tvalid_0's binary_logloss: 0.678825\n",
      "[400]\tvalid_0's auc: 0.719246\tvalid_0's binary_logloss: 0.666008\n",
      "[600]\tvalid_0's auc: 0.739819\tvalid_0's binary_logloss: 0.655404\n",
      "[800]\tvalid_0's auc: 0.753153\tvalid_0's binary_logloss: 0.646048\n",
      "[1000]\tvalid_0's auc: 0.762304\tvalid_0's binary_logloss: 0.638453\n",
      "[1200]\tvalid_0's auc: 0.769171\tvalid_0's binary_logloss: 0.631672\n",
      "[1400]\tvalid_0's auc: 0.775229\tvalid_0's binary_logloss: 0.625257\n",
      "[1600]\tvalid_0's auc: 0.780327\tvalid_0's binary_logloss: 0.619467\n",
      "[1800]\tvalid_0's auc: 0.784031\tvalid_0's binary_logloss: 0.614422\n",
      "[2000]\tvalid_0's auc: 0.786545\tvalid_0's binary_logloss: 0.610463\n",
      "[2200]\tvalid_0's auc: 0.788516\tvalid_0's binary_logloss: 0.606558\n",
      "[2400]\tvalid_0's auc: 0.790078\tvalid_0's binary_logloss: 0.603232\n",
      "[2600]\tvalid_0's auc: 0.791165\tvalid_0's binary_logloss: 0.600339\n",
      "[2800]\tvalid_0's auc: 0.792234\tvalid_0's binary_logloss: 0.59767\n",
      "[3000]\tvalid_0's auc: 0.79292\tvalid_0's binary_logloss: 0.595424\n",
      "[3200]\tvalid_0's auc: 0.793626\tvalid_0's binary_logloss: 0.593341\n",
      "[3400]\tvalid_0's auc: 0.794133\tvalid_0's binary_logloss: 0.591469\n",
      "[3600]\tvalid_0's auc: 0.79471\tvalid_0's binary_logloss: 0.589633\n",
      "[3800]\tvalid_0's auc: 0.795115\tvalid_0's binary_logloss: 0.588128\n",
      "[4000]\tvalid_0's auc: 0.795491\tvalid_0's binary_logloss: 0.58666\n",
      "[4200]\tvalid_0's auc: 0.795906\tvalid_0's binary_logloss: 0.585298\n",
      "[4400]\tvalid_0's auc: 0.796208\tvalid_0's binary_logloss: 0.584143\n",
      "[4600]\tvalid_0's auc: 0.796481\tvalid_0's binary_logloss: 0.583051\n",
      "[4800]\tvalid_0's auc: 0.796743\tvalid_0's binary_logloss: 0.582077\n",
      "[5000]\tvalid_0's auc: 0.796896\tvalid_0's binary_logloss: 0.581238\n",
      "[5200]\tvalid_0's auc: 0.797075\tvalid_0's binary_logloss: 0.58041\n",
      "[5400]\tvalid_0's auc: 0.797214\tvalid_0's binary_logloss: 0.579695\n",
      "[5600]\tvalid_0's auc: 0.797356\tvalid_0's binary_logloss: 0.57907\n",
      "[5800]\tvalid_0's auc: 0.797503\tvalid_0's binary_logloss: 0.578465\n",
      "[6000]\tvalid_0's auc: 0.797571\tvalid_0's binary_logloss: 0.577947\n",
      "[6200]\tvalid_0's auc: 0.797664\tvalid_0's binary_logloss: 0.57749\n",
      "[6400]\tvalid_0's auc: 0.79776\tvalid_0's binary_logloss: 0.577032\n",
      "[6600]\tvalid_0's auc: 0.797825\tvalid_0's binary_logloss: 0.576617\n",
      "[6800]\tvalid_0's auc: 0.797853\tvalid_0's binary_logloss: 0.576309\n",
      "[7000]\tvalid_0's auc: 0.79791\tvalid_0's binary_logloss: 0.575952\n",
      "[7200]\tvalid_0's auc: 0.797977\tvalid_0's binary_logloss: 0.575617\n",
      "[7400]\tvalid_0's auc: 0.798023\tvalid_0's binary_logloss: 0.575302\n",
      "[7600]\tvalid_0's auc: 0.798067\tvalid_0's binary_logloss: 0.575029\n",
      "[7800]\tvalid_0's auc: 0.798112\tvalid_0's binary_logloss: 0.574766\n",
      "[8000]\tvalid_0's auc: 0.798166\tvalid_0's binary_logloss: 0.574535\n",
      "[8200]\tvalid_0's auc: 0.798177\tvalid_0's binary_logloss: 0.574344\n",
      "[8400]\tvalid_0's auc: 0.798212\tvalid_0's binary_logloss: 0.574115\n",
      "[8600]\tvalid_0's auc: 0.798221\tvalid_0's binary_logloss: 0.57394\n",
      "[8800]\tvalid_0's auc: 0.798234\tvalid_0's binary_logloss: 0.573787\n",
      "[9000]\tvalid_0's auc: 0.79823\tvalid_0's binary_logloss: 0.573641\n",
      "[9200]\tvalid_0's auc: 0.798264\tvalid_0's binary_logloss: 0.573461\n",
      "[9400]\tvalid_0's auc: 0.798278\tvalid_0's binary_logloss: 0.573317\n",
      "[9600]\tvalid_0's auc: 0.798295\tvalid_0's binary_logloss: 0.573185\n",
      "[9800]\tvalid_0's auc: 0.79835\tvalid_0's binary_logloss: 0.573031\n",
      "[10000]\tvalid_0's auc: 0.798357\tvalid_0's binary_logloss: 0.572919\n",
      "[10200]\tvalid_0's auc: 0.798374\tvalid_0's binary_logloss: 0.572795\n",
      "[10400]\tvalid_0's auc: 0.798379\tvalid_0's binary_logloss: 0.572696\n",
      "Early stopping, best iteration is:\n",
      "[10275]\tvalid_0's auc: 0.798383\tvalid_0's binary_logloss: 0.572755\n",
      "Batch 4 started...\n",
      "[0]\tvalidation_0-auc:0.51981\n",
      "[200]\tvalidation_0-auc:0.66978\n",
      "[400]\tvalidation_0-auc:0.71665\n",
      "[600]\tvalidation_0-auc:0.73984\n",
      "[800]\tvalidation_0-auc:0.75267\n",
      "[1000]\tvalidation_0-auc:0.76260\n",
      "[1200]\tvalidation_0-auc:0.77033\n",
      "[1400]\tvalidation_0-auc:0.77589\n",
      "[1600]\tvalidation_0-auc:0.78107\n",
      "[1800]\tvalidation_0-auc:0.78531\n",
      "[2000]\tvalidation_0-auc:0.78832\n",
      "[2200]\tvalidation_0-auc:0.79029\n",
      "[2400]\tvalidation_0-auc:0.79195\n",
      "[2600]\tvalidation_0-auc:0.79317\n",
      "[2800]\tvalidation_0-auc:0.79407\n",
      "[3000]\tvalidation_0-auc:0.79487\n",
      "[3200]\tvalidation_0-auc:0.79551\n",
      "[3400]\tvalidation_0-auc:0.79593\n",
      "[3600]\tvalidation_0-auc:0.79641\n",
      "[3800]\tvalidation_0-auc:0.79673\n",
      "[4000]\tvalidation_0-auc:0.79707\n",
      "[4200]\tvalidation_0-auc:0.79739\n",
      "[4400]\tvalidation_0-auc:0.79765\n",
      "[4600]\tvalidation_0-auc:0.79785\n",
      "[4800]\tvalidation_0-auc:0.79807\n",
      "[5000]\tvalidation_0-auc:0.79829\n",
      "[5200]\tvalidation_0-auc:0.79845\n",
      "[5400]\tvalidation_0-auc:0.79864\n",
      "[5600]\tvalidation_0-auc:0.79873\n",
      "[5800]\tvalidation_0-auc:0.79887\n",
      "[6000]\tvalidation_0-auc:0.79902\n",
      "[6200]\tvalidation_0-auc:0.79908\n",
      "[6400]\tvalidation_0-auc:0.79915\n",
      "[6600]\tvalidation_0-auc:0.79924\n",
      "[6800]\tvalidation_0-auc:0.79931\n",
      "[7000]\tvalidation_0-auc:0.79937\n",
      "[7200]\tvalidation_0-auc:0.79938\n",
      "[7272]\tvalidation_0-auc:0.79939\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.671797\tvalid_0's binary_logloss: 0.679098\n",
      "[400]\tvalid_0's auc: 0.717002\tvalid_0's binary_logloss: 0.666154\n",
      "[600]\tvalid_0's auc: 0.739715\tvalid_0's binary_logloss: 0.655564\n",
      "[800]\tvalid_0's auc: 0.752085\tvalid_0's binary_logloss: 0.646739\n",
      "[1000]\tvalid_0's auc: 0.762218\tvalid_0's binary_logloss: 0.638678\n",
      "[1200]\tvalid_0's auc: 0.769822\tvalid_0's binary_logloss: 0.631664\n",
      "[1400]\tvalid_0's auc: 0.775468\tvalid_0's binary_logloss: 0.625346\n",
      "[1600]\tvalid_0's auc: 0.78048\tvalid_0's binary_logloss: 0.619641\n",
      "[1800]\tvalid_0's auc: 0.783711\tvalid_0's binary_logloss: 0.615187\n",
      "[2000]\tvalid_0's auc: 0.786416\tvalid_0's binary_logloss: 0.610892\n",
      "[2200]\tvalid_0's auc: 0.788666\tvalid_0's binary_logloss: 0.607036\n",
      "[2400]\tvalid_0's auc: 0.790496\tvalid_0's binary_logloss: 0.603501\n",
      "[2600]\tvalid_0's auc: 0.791727\tvalid_0's binary_logloss: 0.600604\n",
      "[2800]\tvalid_0's auc: 0.792873\tvalid_0's binary_logloss: 0.597822\n",
      "[3000]\tvalid_0's auc: 0.793787\tvalid_0's binary_logloss: 0.59544\n",
      "[3200]\tvalid_0's auc: 0.794456\tvalid_0's binary_logloss: 0.593437\n",
      "[3400]\tvalid_0's auc: 0.795049\tvalid_0's binary_logloss: 0.591557\n",
      "[3600]\tvalid_0's auc: 0.795735\tvalid_0's binary_logloss: 0.589725\n",
      "[3800]\tvalid_0's auc: 0.796266\tvalid_0's binary_logloss: 0.588121\n",
      "[4000]\tvalid_0's auc: 0.796635\tvalid_0's binary_logloss: 0.586788\n",
      "[4200]\tvalid_0's auc: 0.797042\tvalid_0's binary_logloss: 0.585414\n",
      "[4400]\tvalid_0's auc: 0.79734\tvalid_0's binary_logloss: 0.58424\n",
      "[4600]\tvalid_0's auc: 0.797636\tvalid_0's binary_logloss: 0.583227\n",
      "[4800]\tvalid_0's auc: 0.797911\tvalid_0's binary_logloss: 0.582261\n",
      "[5000]\tvalid_0's auc: 0.798124\tvalid_0's binary_logloss: 0.581427\n",
      "[5200]\tvalid_0's auc: 0.798352\tvalid_0's binary_logloss: 0.580616\n",
      "[5400]\tvalid_0's auc: 0.798505\tvalid_0's binary_logloss: 0.579945\n",
      "[5600]\tvalid_0's auc: 0.798703\tvalid_0's binary_logloss: 0.579252\n",
      "[5800]\tvalid_0's auc: 0.798909\tvalid_0's binary_logloss: 0.578581\n",
      "[6000]\tvalid_0's auc: 0.79906\tvalid_0's binary_logloss: 0.578015\n",
      "[6200]\tvalid_0's auc: 0.799174\tvalid_0's binary_logloss: 0.577503\n",
      "[6400]\tvalid_0's auc: 0.799287\tvalid_0's binary_logloss: 0.577062\n",
      "[6600]\tvalid_0's auc: 0.799401\tvalid_0's binary_logloss: 0.576628\n",
      "[6800]\tvalid_0's auc: 0.799463\tvalid_0's binary_logloss: 0.57627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7000]\tvalid_0's auc: 0.799553\tvalid_0's binary_logloss: 0.575919\n",
      "[7200]\tvalid_0's auc: 0.799623\tvalid_0's binary_logloss: 0.575596\n",
      "[7400]\tvalid_0's auc: 0.799693\tvalid_0's binary_logloss: 0.575285\n",
      "[7600]\tvalid_0's auc: 0.799766\tvalid_0's binary_logloss: 0.574993\n",
      "[7800]\tvalid_0's auc: 0.799843\tvalid_0's binary_logloss: 0.574726\n",
      "[8000]\tvalid_0's auc: 0.79993\tvalid_0's binary_logloss: 0.574477\n",
      "[8200]\tvalid_0's auc: 0.799965\tvalid_0's binary_logloss: 0.574278\n",
      "[8400]\tvalid_0's auc: 0.800014\tvalid_0's binary_logloss: 0.574087\n",
      "[8600]\tvalid_0's auc: 0.800036\tvalid_0's binary_logloss: 0.573884\n",
      "[8800]\tvalid_0's auc: 0.800123\tvalid_0's binary_logloss: 0.573676\n",
      "[9000]\tvalid_0's auc: 0.800125\tvalid_0's binary_logloss: 0.573524\n",
      "Early stopping, best iteration is:\n",
      "[8865]\tvalid_0's auc: 0.800141\tvalid_0's binary_logloss: 0.573611\n",
      "Batch 5 started...\n",
      "[0]\tvalidation_0-auc:0.51598\n",
      "[200]\tvalidation_0-auc:0.67118\n",
      "[400]\tvalidation_0-auc:0.71423\n",
      "[600]\tvalidation_0-auc:0.73565\n",
      "[800]\tvalidation_0-auc:0.74829\n",
      "[1000]\tvalidation_0-auc:0.75915\n",
      "[1200]\tvalidation_0-auc:0.76647\n",
      "[1400]\tvalidation_0-auc:0.77249\n",
      "[1600]\tvalidation_0-auc:0.77759\n",
      "[1800]\tvalidation_0-auc:0.78195\n",
      "[2000]\tvalidation_0-auc:0.78570\n",
      "[2200]\tvalidation_0-auc:0.78798\n",
      "[2400]\tvalidation_0-auc:0.78976\n",
      "[2600]\tvalidation_0-auc:0.79117\n",
      "[2800]\tvalidation_0-auc:0.79226\n",
      "[3000]\tvalidation_0-auc:0.79320\n",
      "[3200]\tvalidation_0-auc:0.79395\n",
      "[3400]\tvalidation_0-auc:0.79453\n",
      "[3600]\tvalidation_0-auc:0.79511\n",
      "[3800]\tvalidation_0-auc:0.79552\n",
      "[4000]\tvalidation_0-auc:0.79598\n",
      "[4200]\tvalidation_0-auc:0.79627\n",
      "[4400]\tvalidation_0-auc:0.79656\n",
      "[4600]\tvalidation_0-auc:0.79688\n",
      "[4800]\tvalidation_0-auc:0.79711\n",
      "[5000]\tvalidation_0-auc:0.79735\n",
      "[5200]\tvalidation_0-auc:0.79754\n",
      "[5400]\tvalidation_0-auc:0.79771\n",
      "[5600]\tvalidation_0-auc:0.79787\n",
      "[5800]\tvalidation_0-auc:0.79800\n",
      "[6000]\tvalidation_0-auc:0.79813\n",
      "[6200]\tvalidation_0-auc:0.79824\n",
      "[6400]\tvalidation_0-auc:0.79836\n",
      "[6600]\tvalidation_0-auc:0.79850\n",
      "[6800]\tvalidation_0-auc:0.79863\n",
      "[7000]\tvalidation_0-auc:0.79869\n",
      "[7200]\tvalidation_0-auc:0.79878\n",
      "[7400]\tvalidation_0-auc:0.79885\n",
      "[7600]\tvalidation_0-auc:0.79891\n",
      "[7800]\tvalidation_0-auc:0.79898\n",
      "[8000]\tvalidation_0-auc:0.79904\n",
      "[8200]\tvalidation_0-auc:0.79905\n",
      "[8400]\tvalidation_0-auc:0.79910\n",
      "[8600]\tvalidation_0-auc:0.79915\n",
      "[8800]\tvalidation_0-auc:0.79920\n",
      "[9000]\tvalidation_0-auc:0.79922\n",
      "[9200]\tvalidation_0-auc:0.79924\n",
      "[9400]\tvalidation_0-auc:0.79927\n",
      "[9600]\tvalidation_0-auc:0.79930\n",
      "[9800]\tvalidation_0-auc:0.79931\n",
      "[10000]\tvalidation_0-auc:0.79934\n",
      "[10200]\tvalidation_0-auc:0.79933\n",
      "[10253]\tvalidation_0-auc:0.79934\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.674626\tvalid_0's binary_logloss: 0.67882\n",
      "[400]\tvalid_0's auc: 0.716286\tvalid_0's binary_logloss: 0.666391\n",
      "[600]\tvalid_0's auc: 0.738727\tvalid_0's binary_logloss: 0.655862\n",
      "[800]\tvalid_0's auc: 0.751243\tvalid_0's binary_logloss: 0.64661\n",
      "[1000]\tvalid_0's auc: 0.759236\tvalid_0's binary_logloss: 0.639392\n",
      "[1200]\tvalid_0's auc: 0.766547\tvalid_0's binary_logloss: 0.632692\n",
      "[1400]\tvalid_0's auc: 0.773243\tvalid_0's binary_logloss: 0.626395\n",
      "[1600]\tvalid_0's auc: 0.77812\tvalid_0's binary_logloss: 0.620939\n",
      "[1800]\tvalid_0's auc: 0.782023\tvalid_0's binary_logloss: 0.616005\n",
      "[2000]\tvalid_0's auc: 0.784916\tvalid_0's binary_logloss: 0.611691\n",
      "[2200]\tvalid_0's auc: 0.787103\tvalid_0's binary_logloss: 0.607815\n",
      "[2400]\tvalid_0's auc: 0.788751\tvalid_0's binary_logloss: 0.604482\n",
      "[2600]\tvalid_0's auc: 0.790288\tvalid_0's binary_logloss: 0.601231\n",
      "[2800]\tvalid_0's auc: 0.791516\tvalid_0's binary_logloss: 0.598379\n",
      "[3000]\tvalid_0's auc: 0.792556\tvalid_0's binary_logloss: 0.595921\n",
      "[3200]\tvalid_0's auc: 0.793273\tvalid_0's binary_logloss: 0.593759\n",
      "[3400]\tvalid_0's auc: 0.793859\tvalid_0's binary_logloss: 0.591952\n",
      "[3600]\tvalid_0's auc: 0.79454\tvalid_0's binary_logloss: 0.590048\n",
      "[3800]\tvalid_0's auc: 0.795021\tvalid_0's binary_logloss: 0.588523\n",
      "[4000]\tvalid_0's auc: 0.795429\tvalid_0's binary_logloss: 0.587119\n",
      "[4200]\tvalid_0's auc: 0.795834\tvalid_0's binary_logloss: 0.585836\n",
      "[4400]\tvalid_0's auc: 0.796228\tvalid_0's binary_logloss: 0.584632\n",
      "[4600]\tvalid_0's auc: 0.796563\tvalid_0's binary_logloss: 0.583578\n",
      "[4800]\tvalid_0's auc: 0.796818\tvalid_0's binary_logloss: 0.582623\n",
      "[5000]\tvalid_0's auc: 0.797065\tvalid_0's binary_logloss: 0.581755\n",
      "[5200]\tvalid_0's auc: 0.797328\tvalid_0's binary_logloss: 0.580895\n",
      "[5400]\tvalid_0's auc: 0.797544\tvalid_0's binary_logloss: 0.580136\n",
      "[5600]\tvalid_0's auc: 0.797684\tvalid_0's binary_logloss: 0.579524\n",
      "[5800]\tvalid_0's auc: 0.797896\tvalid_0's binary_logloss: 0.578835\n",
      "[6000]\tvalid_0's auc: 0.798091\tvalid_0's binary_logloss: 0.578201\n",
      "[6200]\tvalid_0's auc: 0.798277\tvalid_0's binary_logloss: 0.577605\n",
      "[6400]\tvalid_0's auc: 0.798434\tvalid_0's binary_logloss: 0.577122\n",
      "[6600]\tvalid_0's auc: 0.798559\tvalid_0's binary_logloss: 0.576684\n",
      "[6800]\tvalid_0's auc: 0.79868\tvalid_0's binary_logloss: 0.576308\n",
      "[7000]\tvalid_0's auc: 0.798777\tvalid_0's binary_logloss: 0.575954\n",
      "[7200]\tvalid_0's auc: 0.79887\tvalid_0's binary_logloss: 0.575631\n",
      "[7400]\tvalid_0's auc: 0.798924\tvalid_0's binary_logloss: 0.575337\n",
      "[7600]\tvalid_0's auc: 0.799011\tvalid_0's binary_logloss: 0.575017\n",
      "[7800]\tvalid_0's auc: 0.799081\tvalid_0's binary_logloss: 0.574737\n",
      "[8000]\tvalid_0's auc: 0.799161\tvalid_0's binary_logloss: 0.574468\n",
      "[8200]\tvalid_0's auc: 0.799212\tvalid_0's binary_logloss: 0.574242\n",
      "[8400]\tvalid_0's auc: 0.799253\tvalid_0's binary_logloss: 0.574019\n",
      "[8600]\tvalid_0's auc: 0.799271\tvalid_0's binary_logloss: 0.573859\n",
      "[8800]\tvalid_0's auc: 0.799294\tvalid_0's binary_logloss: 0.573698\n",
      "[9000]\tvalid_0's auc: 0.79933\tvalid_0's binary_logloss: 0.57352\n",
      "[9200]\tvalid_0's auc: 0.799355\tvalid_0's binary_logloss: 0.573357\n",
      "[9400]\tvalid_0's auc: 0.799421\tvalid_0's binary_logloss: 0.573201\n",
      "[9600]\tvalid_0's auc: 0.799452\tvalid_0's binary_logloss: 0.573061\n",
      "[9800]\tvalid_0's auc: 0.799479\tvalid_0's binary_logloss: 0.572933\n",
      "[10000]\tvalid_0's auc: 0.799511\tvalid_0's binary_logloss: 0.572797\n",
      "Early stopping, best iteration is:\n",
      "[9936]\tvalid_0's auc: 0.799533\tvalid_0's binary_logloss: 0.572826\n",
      "Batch 6 started...\n",
      "[0]\tvalidation_0-auc:0.51578\n",
      "[200]\tvalidation_0-auc:0.67218\n",
      "[400]\tvalidation_0-auc:0.71462\n",
      "[600]\tvalidation_0-auc:0.73713\n",
      "[800]\tvalidation_0-auc:0.75070\n",
      "[1000]\tvalidation_0-auc:0.76153\n",
      "[1200]\tvalidation_0-auc:0.76971\n",
      "[1400]\tvalidation_0-auc:0.77516\n",
      "[1600]\tvalidation_0-auc:0.77975\n",
      "[1800]\tvalidation_0-auc:0.78347\n",
      "[2000]\tvalidation_0-auc:0.78645\n",
      "[2200]\tvalidation_0-auc:0.78876\n",
      "[2400]\tvalidation_0-auc:0.79023\n",
      "[2600]\tvalidation_0-auc:0.79137\n",
      "[2800]\tvalidation_0-auc:0.79237\n",
      "[3000]\tvalidation_0-auc:0.79301\n",
      "[3200]\tvalidation_0-auc:0.79366\n",
      "[3400]\tvalidation_0-auc:0.79417\n",
      "[3600]\tvalidation_0-auc:0.79460\n",
      "[3800]\tvalidation_0-auc:0.79497\n",
      "[4000]\tvalidation_0-auc:0.79523\n",
      "[4200]\tvalidation_0-auc:0.79552\n",
      "[4400]\tvalidation_0-auc:0.79580\n",
      "[4600]\tvalidation_0-auc:0.79604\n",
      "[4800]\tvalidation_0-auc:0.79623\n",
      "[5000]\tvalidation_0-auc:0.79643\n",
      "[5200]\tvalidation_0-auc:0.79659\n",
      "[5400]\tvalidation_0-auc:0.79675\n",
      "[5600]\tvalidation_0-auc:0.79688\n",
      "[5800]\tvalidation_0-auc:0.79697\n",
      "[6000]\tvalidation_0-auc:0.79711\n",
      "[6200]\tvalidation_0-auc:0.79720\n",
      "[6400]\tvalidation_0-auc:0.79729\n",
      "[6600]\tvalidation_0-auc:0.79738\n",
      "[6800]\tvalidation_0-auc:0.79747\n",
      "[7000]\tvalidation_0-auc:0.79754\n",
      "[7200]\tvalidation_0-auc:0.79760\n",
      "[7400]\tvalidation_0-auc:0.79764\n",
      "[7600]\tvalidation_0-auc:0.79770\n",
      "[7800]\tvalidation_0-auc:0.79773\n",
      "[8000]\tvalidation_0-auc:0.79780\n",
      "[8200]\tvalidation_0-auc:0.79782\n",
      "[8400]\tvalidation_0-auc:0.79784\n",
      "[8600]\tvalidation_0-auc:0.79786\n",
      "[8800]\tvalidation_0-auc:0.79788\n",
      "[9000]\tvalidation_0-auc:0.79792\n",
      "[9200]\tvalidation_0-auc:0.79794\n",
      "[9400]\tvalidation_0-auc:0.79796\n",
      "[9600]\tvalidation_0-auc:0.79800\n",
      "[9800]\tvalidation_0-auc:0.79803\n",
      "[10000]\tvalidation_0-auc:0.79807\n",
      "[10200]\tvalidation_0-auc:0.79811\n",
      "[10387]\tvalidation_0-auc:0.79811\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.672135\tvalid_0's binary_logloss: 0.679135\n",
      "[400]\tvalid_0's auc: 0.716825\tvalid_0's binary_logloss: 0.666347\n",
      "[600]\tvalid_0's auc: 0.737992\tvalid_0's binary_logloss: 0.655453\n",
      "[800]\tvalid_0's auc: 0.752734\tvalid_0's binary_logloss: 0.645852\n",
      "[1000]\tvalid_0's auc: 0.761214\tvalid_0's binary_logloss: 0.637981\n",
      "[1200]\tvalid_0's auc: 0.768683\tvalid_0's binary_logloss: 0.63108\n",
      "[1400]\tvalid_0's auc: 0.774616\tvalid_0's binary_logloss: 0.624955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1600]\tvalid_0's auc: 0.779589\tvalid_0's binary_logloss: 0.619171\n",
      "[1800]\tvalid_0's auc: 0.782822\tvalid_0's binary_logloss: 0.614448\n",
      "[2000]\tvalid_0's auc: 0.785101\tvalid_0's binary_logloss: 0.610475\n",
      "[2200]\tvalid_0's auc: 0.787236\tvalid_0's binary_logloss: 0.606647\n",
      "[2400]\tvalid_0's auc: 0.788924\tvalid_0's binary_logloss: 0.603384\n",
      "[2600]\tvalid_0's auc: 0.790039\tvalid_0's binary_logloss: 0.600586\n",
      "[2800]\tvalid_0's auc: 0.791183\tvalid_0's binary_logloss: 0.597898\n",
      "[3000]\tvalid_0's auc: 0.792029\tvalid_0's binary_logloss: 0.595624\n",
      "[3200]\tvalid_0's auc: 0.792843\tvalid_0's binary_logloss: 0.593394\n",
      "[3400]\tvalid_0's auc: 0.793503\tvalid_0's binary_logloss: 0.591461\n",
      "[3600]\tvalid_0's auc: 0.794078\tvalid_0's binary_logloss: 0.589742\n",
      "[3800]\tvalid_0's auc: 0.794568\tvalid_0's binary_logloss: 0.588194\n",
      "[4000]\tvalid_0's auc: 0.794881\tvalid_0's binary_logloss: 0.586925\n",
      "[4200]\tvalid_0's auc: 0.79524\tvalid_0's binary_logloss: 0.58561\n",
      "[4400]\tvalid_0's auc: 0.79556\tvalid_0's binary_logloss: 0.584462\n",
      "[4600]\tvalid_0's auc: 0.795857\tvalid_0's binary_logloss: 0.583401\n",
      "[4800]\tvalid_0's auc: 0.796144\tvalid_0's binary_logloss: 0.582458\n",
      "[5000]\tvalid_0's auc: 0.796344\tvalid_0's binary_logloss: 0.581589\n",
      "[5200]\tvalid_0's auc: 0.796475\tvalid_0's binary_logloss: 0.580901\n",
      "[5400]\tvalid_0's auc: 0.796593\tvalid_0's binary_logloss: 0.580259\n",
      "[5600]\tvalid_0's auc: 0.796772\tvalid_0's binary_logloss: 0.579568\n",
      "[5800]\tvalid_0's auc: 0.796918\tvalid_0's binary_logloss: 0.578977\n",
      "[6000]\tvalid_0's auc: 0.797052\tvalid_0's binary_logloss: 0.578422\n",
      "[6200]\tvalid_0's auc: 0.797194\tvalid_0's binary_logloss: 0.577893\n",
      "[6400]\tvalid_0's auc: 0.797355\tvalid_0's binary_logloss: 0.577369\n",
      "[6600]\tvalid_0's auc: 0.797393\tvalid_0's binary_logloss: 0.576975\n",
      "[6800]\tvalid_0's auc: 0.797469\tvalid_0's binary_logloss: 0.576596\n",
      "[7000]\tvalid_0's auc: 0.797552\tvalid_0's binary_logloss: 0.576231\n",
      "[7200]\tvalid_0's auc: 0.797611\tvalid_0's binary_logloss: 0.575913\n",
      "[7400]\tvalid_0's auc: 0.797692\tvalid_0's binary_logloss: 0.575593\n",
      "[7600]\tvalid_0's auc: 0.797748\tvalid_0's binary_logloss: 0.57535\n",
      "[7800]\tvalid_0's auc: 0.797838\tvalid_0's binary_logloss: 0.575052\n",
      "[8000]\tvalid_0's auc: 0.797869\tvalid_0's binary_logloss: 0.574814\n",
      "[8200]\tvalid_0's auc: 0.797906\tvalid_0's binary_logloss: 0.574623\n",
      "[8400]\tvalid_0's auc: 0.797951\tvalid_0's binary_logloss: 0.57442\n",
      "[8600]\tvalid_0's auc: 0.798009\tvalid_0's binary_logloss: 0.574218\n",
      "[8800]\tvalid_0's auc: 0.798076\tvalid_0's binary_logloss: 0.574031\n",
      "[9000]\tvalid_0's auc: 0.798101\tvalid_0's binary_logloss: 0.573874\n",
      "[9200]\tvalid_0's auc: 0.798133\tvalid_0's binary_logloss: 0.573735\n",
      "[9400]\tvalid_0's auc: 0.798194\tvalid_0's binary_logloss: 0.573568\n",
      "[9600]\tvalid_0's auc: 0.798232\tvalid_0's binary_logloss: 0.573425\n",
      "[9800]\tvalid_0's auc: 0.798243\tvalid_0's binary_logloss: 0.573313\n",
      "[10000]\tvalid_0's auc: 0.798286\tvalid_0's binary_logloss: 0.573171\n",
      "[10200]\tvalid_0's auc: 0.798304\tvalid_0's binary_logloss: 0.573061\n",
      "[10400]\tvalid_0's auc: 0.798322\tvalid_0's binary_logloss: 0.572957\n",
      "Early stopping, best iteration is:\n",
      "[10320]\tvalid_0's auc: 0.798332\tvalid_0's binary_logloss: 0.572991\n",
      "Batch 7 started...\n",
      "[0]\tvalidation_0-auc:0.51854\n",
      "[200]\tvalidation_0-auc:0.67480\n",
      "[400]\tvalidation_0-auc:0.71412\n",
      "[600]\tvalidation_0-auc:0.73523\n",
      "[800]\tvalidation_0-auc:0.75021\n",
      "[1000]\tvalidation_0-auc:0.76068\n",
      "[1200]\tvalidation_0-auc:0.76750\n",
      "[1400]\tvalidation_0-auc:0.77353\n",
      "[1600]\tvalidation_0-auc:0.77888\n",
      "[1800]\tvalidation_0-auc:0.78257\n",
      "[2000]\tvalidation_0-auc:0.78568\n",
      "[2200]\tvalidation_0-auc:0.78766\n",
      "[2400]\tvalidation_0-auc:0.78937\n",
      "[2600]\tvalidation_0-auc:0.79065\n",
      "[2800]\tvalidation_0-auc:0.79161\n",
      "[3000]\tvalidation_0-auc:0.79234\n",
      "[3200]\tvalidation_0-auc:0.79299\n",
      "[3400]\tvalidation_0-auc:0.79353\n",
      "[3600]\tvalidation_0-auc:0.79399\n",
      "[3800]\tvalidation_0-auc:0.79432\n",
      "[4000]\tvalidation_0-auc:0.79468\n",
      "[4200]\tvalidation_0-auc:0.79495\n",
      "[4400]\tvalidation_0-auc:0.79516\n",
      "[4600]\tvalidation_0-auc:0.79536\n",
      "[4800]\tvalidation_0-auc:0.79550\n",
      "[5000]\tvalidation_0-auc:0.79566\n",
      "[5200]\tvalidation_0-auc:0.79580\n",
      "[5400]\tvalidation_0-auc:0.79593\n",
      "[5600]\tvalidation_0-auc:0.79607\n",
      "[5800]\tvalidation_0-auc:0.79618\n",
      "[6000]\tvalidation_0-auc:0.79629\n",
      "[6200]\tvalidation_0-auc:0.79639\n",
      "[6400]\tvalidation_0-auc:0.79648\n",
      "[6600]\tvalidation_0-auc:0.79656\n",
      "[6800]\tvalidation_0-auc:0.79663\n",
      "[7000]\tvalidation_0-auc:0.79669\n",
      "[7200]\tvalidation_0-auc:0.79674\n",
      "[7400]\tvalidation_0-auc:0.79680\n",
      "[7600]\tvalidation_0-auc:0.79685\n",
      "[7800]\tvalidation_0-auc:0.79690\n",
      "[8000]\tvalidation_0-auc:0.79696\n",
      "[8200]\tvalidation_0-auc:0.79701\n",
      "[8400]\tvalidation_0-auc:0.79704\n",
      "[8600]\tvalidation_0-auc:0.79708\n",
      "[8800]\tvalidation_0-auc:0.79711\n",
      "[9000]\tvalidation_0-auc:0.79714\n",
      "[9200]\tvalidation_0-auc:0.79720\n",
      "[9400]\tvalidation_0-auc:0.79724\n",
      "[9600]\tvalidation_0-auc:0.79725\n",
      "[9698]\tvalidation_0-auc:0.79725\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.673379\tvalid_0's binary_logloss: 0.679402\n",
      "[400]\tvalid_0's auc: 0.71568\tvalid_0's binary_logloss: 0.667053\n",
      "[600]\tvalid_0's auc: 0.738614\tvalid_0's binary_logloss: 0.655699\n",
      "[800]\tvalid_0's auc: 0.751335\tvalid_0's binary_logloss: 0.646878\n",
      "[1000]\tvalid_0's auc: 0.761183\tvalid_0's binary_logloss: 0.638832\n",
      "[1200]\tvalid_0's auc: 0.768174\tvalid_0's binary_logloss: 0.632231\n",
      "[1400]\tvalid_0's auc: 0.774501\tvalid_0's binary_logloss: 0.626052\n",
      "[1600]\tvalid_0's auc: 0.779022\tvalid_0's binary_logloss: 0.620657\n",
      "[1800]\tvalid_0's auc: 0.782341\tvalid_0's binary_logloss: 0.61594\n",
      "[2000]\tvalid_0's auc: 0.785178\tvalid_0's binary_logloss: 0.611604\n",
      "[2200]\tvalid_0's auc: 0.787114\tvalid_0's binary_logloss: 0.607945\n",
      "[2400]\tvalid_0's auc: 0.788744\tvalid_0's binary_logloss: 0.604613\n",
      "[2600]\tvalid_0's auc: 0.789684\tvalid_0's binary_logloss: 0.601995\n",
      "[2800]\tvalid_0's auc: 0.790844\tvalid_0's binary_logloss: 0.599231\n",
      "[3000]\tvalid_0's auc: 0.791818\tvalid_0's binary_logloss: 0.596758\n",
      "[3200]\tvalid_0's auc: 0.79258\tvalid_0's binary_logloss: 0.594616\n",
      "[3400]\tvalid_0's auc: 0.793157\tvalid_0's binary_logloss: 0.592694\n",
      "[3600]\tvalid_0's auc: 0.793616\tvalid_0's binary_logloss: 0.591056\n",
      "[3800]\tvalid_0's auc: 0.793952\tvalid_0's binary_logloss: 0.589683\n",
      "[4000]\tvalid_0's auc: 0.794399\tvalid_0's binary_logloss: 0.588182\n",
      "[4200]\tvalid_0's auc: 0.79472\tvalid_0's binary_logloss: 0.586944\n",
      "[4400]\tvalid_0's auc: 0.795017\tvalid_0's binary_logloss: 0.585784\n",
      "[4600]\tvalid_0's auc: 0.795261\tvalid_0's binary_logloss: 0.58482\n",
      "[4800]\tvalid_0's auc: 0.795486\tvalid_0's binary_logloss: 0.583878\n",
      "[5000]\tvalid_0's auc: 0.795752\tvalid_0's binary_logloss: 0.582966\n",
      "[5200]\tvalid_0's auc: 0.795881\tvalid_0's binary_logloss: 0.582293\n",
      "[5400]\tvalid_0's auc: 0.795984\tvalid_0's binary_logloss: 0.581671\n",
      "[5600]\tvalid_0's auc: 0.796187\tvalid_0's binary_logloss: 0.580947\n",
      "[5800]\tvalid_0's auc: 0.796303\tvalid_0's binary_logloss: 0.580373\n",
      "[6000]\tvalid_0's auc: 0.796437\tvalid_0's binary_logloss: 0.579841\n",
      "[6200]\tvalid_0's auc: 0.796598\tvalid_0's binary_logloss: 0.579319\n",
      "[6400]\tvalid_0's auc: 0.796721\tvalid_0's binary_logloss: 0.578846\n",
      "[6600]\tvalid_0's auc: 0.796865\tvalid_0's binary_logloss: 0.578395\n",
      "[6800]\tvalid_0's auc: 0.796952\tvalid_0's binary_logloss: 0.578045\n",
      "[7000]\tvalid_0's auc: 0.797069\tvalid_0's binary_logloss: 0.577669\n",
      "[7200]\tvalid_0's auc: 0.797112\tvalid_0's binary_logloss: 0.577362\n",
      "[7400]\tvalid_0's auc: 0.797152\tvalid_0's binary_logloss: 0.577059\n",
      "[7600]\tvalid_0's auc: 0.797169\tvalid_0's binary_logloss: 0.576816\n",
      "[7800]\tvalid_0's auc: 0.797219\tvalid_0's binary_logloss: 0.576572\n",
      "[8000]\tvalid_0's auc: 0.797284\tvalid_0's binary_logloss: 0.576324\n",
      "[8200]\tvalid_0's auc: 0.797358\tvalid_0's binary_logloss: 0.576082\n",
      "[8400]\tvalid_0's auc: 0.797404\tvalid_0's binary_logloss: 0.5759\n",
      "[8600]\tvalid_0's auc: 0.79743\tvalid_0's binary_logloss: 0.575707\n",
      "[8800]\tvalid_0's auc: 0.79747\tvalid_0's binary_logloss: 0.575534\n",
      "[9000]\tvalid_0's auc: 0.79754\tvalid_0's binary_logloss: 0.575339\n",
      "[9200]\tvalid_0's auc: 0.797581\tvalid_0's binary_logloss: 0.575184\n",
      "[9400]\tvalid_0's auc: 0.797592\tvalid_0's binary_logloss: 0.575063\n",
      "[9600]\tvalid_0's auc: 0.797611\tvalid_0's binary_logloss: 0.574941\n",
      "[9800]\tvalid_0's auc: 0.79764\tvalid_0's binary_logloss: 0.574802\n",
      "[10000]\tvalid_0's auc: 0.797681\tvalid_0's binary_logloss: 0.574692\n",
      "[10200]\tvalid_0's auc: 0.797664\tvalid_0's binary_logloss: 0.574614\n",
      "Early stopping, best iteration is:\n",
      "[10041]\tvalid_0's auc: 0.797693\tvalid_0's binary_logloss: 0.574671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 started...\n",
      "[0]\tvalidation_0-auc:0.51927\n",
      "[200]\tvalidation_0-auc:0.66812\n",
      "[400]\tvalidation_0-auc:0.71177\n",
      "[600]\tvalidation_0-auc:0.73547\n",
      "[800]\tvalidation_0-auc:0.74903\n",
      "[1000]\tvalidation_0-auc:0.75982\n",
      "[1200]\tvalidation_0-auc:0.76842\n",
      "[1400]\tvalidation_0-auc:0.77435\n",
      "[1600]\tvalidation_0-auc:0.77929\n",
      "[1800]\tvalidation_0-auc:0.78308\n",
      "[2000]\tvalidation_0-auc:0.78598\n",
      "[2200]\tvalidation_0-auc:0.78800\n",
      "[2400]\tvalidation_0-auc:0.78951\n",
      "[2600]\tvalidation_0-auc:0.79070\n",
      "[2800]\tvalidation_0-auc:0.79162\n",
      "[3000]\tvalidation_0-auc:0.79237\n",
      "[3200]\tvalidation_0-auc:0.79292\n",
      "[3400]\tvalidation_0-auc:0.79344\n",
      "[3600]\tvalidation_0-auc:0.79383\n",
      "[3800]\tvalidation_0-auc:0.79428\n",
      "[4000]\tvalidation_0-auc:0.79468\n",
      "[4200]\tvalidation_0-auc:0.79497\n",
      "[4400]\tvalidation_0-auc:0.79524\n",
      "[4600]\tvalidation_0-auc:0.79551\n",
      "[4800]\tvalidation_0-auc:0.79570\n",
      "[5000]\tvalidation_0-auc:0.79589\n",
      "[5200]\tvalidation_0-auc:0.79604\n",
      "[5400]\tvalidation_0-auc:0.79618\n",
      "[5600]\tvalidation_0-auc:0.79627\n",
      "[5800]\tvalidation_0-auc:0.79639\n",
      "[6000]\tvalidation_0-auc:0.79650\n",
      "[6200]\tvalidation_0-auc:0.79659\n",
      "[6400]\tvalidation_0-auc:0.79665\n",
      "[6600]\tvalidation_0-auc:0.79671\n",
      "[6800]\tvalidation_0-auc:0.79678\n",
      "[7000]\tvalidation_0-auc:0.79685\n",
      "[7200]\tvalidation_0-auc:0.79689\n",
      "[7400]\tvalidation_0-auc:0.79692\n",
      "[7600]\tvalidation_0-auc:0.79696\n",
      "[7800]\tvalidation_0-auc:0.79700\n",
      "[8000]\tvalidation_0-auc:0.79705\n",
      "[8200]\tvalidation_0-auc:0.79706\n",
      "[8400]\tvalidation_0-auc:0.79709\n",
      "[8600]\tvalidation_0-auc:0.79712\n",
      "[8800]\tvalidation_0-auc:0.79714\n",
      "[9000]\tvalidation_0-auc:0.79712\n",
      "[9065]\tvalidation_0-auc:0.79713\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.672684\tvalid_0's binary_logloss: 0.679315\n",
      "[400]\tvalid_0's auc: 0.71526\tvalid_0's binary_logloss: 0.666891\n",
      "[600]\tvalid_0's auc: 0.737049\tvalid_0's binary_logloss: 0.65622\n",
      "[800]\tvalid_0's auc: 0.749683\tvalid_0's binary_logloss: 0.6472\n",
      "[1000]\tvalid_0's auc: 0.76119\tvalid_0's binary_logloss: 0.639058\n",
      "[1200]\tvalid_0's auc: 0.769211\tvalid_0's binary_logloss: 0.63169\n",
      "[1400]\tvalid_0's auc: 0.77533\tvalid_0's binary_logloss: 0.625251\n",
      "[1600]\tvalid_0's auc: 0.779371\tvalid_0's binary_logloss: 0.620087\n",
      "[1800]\tvalid_0's auc: 0.782532\tvalid_0's binary_logloss: 0.615375\n",
      "[2000]\tvalid_0's auc: 0.784779\tvalid_0's binary_logloss: 0.611369\n",
      "[2200]\tvalid_0's auc: 0.786871\tvalid_0's binary_logloss: 0.607441\n",
      "[2400]\tvalid_0's auc: 0.788099\tvalid_0's binary_logloss: 0.60437\n",
      "[2600]\tvalid_0's auc: 0.789329\tvalid_0's binary_logloss: 0.601458\n",
      "[2800]\tvalid_0's auc: 0.790368\tvalid_0's binary_logloss: 0.598778\n",
      "[3000]\tvalid_0's auc: 0.791212\tvalid_0's binary_logloss: 0.596391\n",
      "[3200]\tvalid_0's auc: 0.791875\tvalid_0's binary_logloss: 0.594325\n",
      "[3400]\tvalid_0's auc: 0.792447\tvalid_0's binary_logloss: 0.592537\n",
      "[3600]\tvalid_0's auc: 0.79288\tvalid_0's binary_logloss: 0.59083\n",
      "[3800]\tvalid_0's auc: 0.793422\tvalid_0's binary_logloss: 0.58917\n",
      "[4000]\tvalid_0's auc: 0.793814\tvalid_0's binary_logloss: 0.587753\n",
      "[4200]\tvalid_0's auc: 0.794277\tvalid_0's binary_logloss: 0.586425\n",
      "[4400]\tvalid_0's auc: 0.794595\tvalid_0's binary_logloss: 0.585292\n",
      "[4600]\tvalid_0's auc: 0.794809\tvalid_0's binary_logloss: 0.58432\n",
      "[4800]\tvalid_0's auc: 0.795082\tvalid_0's binary_logloss: 0.583366\n",
      "[5000]\tvalid_0's auc: 0.795353\tvalid_0's binary_logloss: 0.582432\n",
      "[5200]\tvalid_0's auc: 0.795568\tvalid_0's binary_logloss: 0.581606\n",
      "[5400]\tvalid_0's auc: 0.795749\tvalid_0's binary_logloss: 0.58092\n",
      "[5600]\tvalid_0's auc: 0.79588\tvalid_0's binary_logloss: 0.580332\n",
      "[5800]\tvalid_0's auc: 0.796017\tvalid_0's binary_logloss: 0.579768\n",
      "[6000]\tvalid_0's auc: 0.796152\tvalid_0's binary_logloss: 0.579193\n",
      "[6200]\tvalid_0's auc: 0.796259\tvalid_0's binary_logloss: 0.578707\n",
      "[6400]\tvalid_0's auc: 0.796351\tvalid_0's binary_logloss: 0.578258\n",
      "[6600]\tvalid_0's auc: 0.796436\tvalid_0's binary_logloss: 0.577838\n",
      "[6800]\tvalid_0's auc: 0.796592\tvalid_0's binary_logloss: 0.577404\n",
      "[7000]\tvalid_0's auc: 0.796687\tvalid_0's binary_logloss: 0.577064\n",
      "[7200]\tvalid_0's auc: 0.796773\tvalid_0's binary_logloss: 0.576739\n",
      "[7400]\tvalid_0's auc: 0.796859\tvalid_0's binary_logloss: 0.576417\n",
      "[7600]\tvalid_0's auc: 0.796901\tvalid_0's binary_logloss: 0.576151\n",
      "[7800]\tvalid_0's auc: 0.79696\tvalid_0's binary_logloss: 0.575901\n",
      "[8000]\tvalid_0's auc: 0.797039\tvalid_0's binary_logloss: 0.575641\n",
      "[8200]\tvalid_0's auc: 0.797116\tvalid_0's binary_logloss: 0.575404\n",
      "[8400]\tvalid_0's auc: 0.797179\tvalid_0's binary_logloss: 0.575194\n",
      "[8600]\tvalid_0's auc: 0.797231\tvalid_0's binary_logloss: 0.574995\n",
      "[8800]\tvalid_0's auc: 0.797286\tvalid_0's binary_logloss: 0.574817\n",
      "[9000]\tvalid_0's auc: 0.79731\tvalid_0's binary_logloss: 0.574663\n",
      "[9200]\tvalid_0's auc: 0.797353\tvalid_0's binary_logloss: 0.574504\n",
      "[9400]\tvalid_0's auc: 0.797367\tvalid_0's binary_logloss: 0.574384\n",
      "[9600]\tvalid_0's auc: 0.7974\tvalid_0's binary_logloss: 0.574244\n",
      "[9800]\tvalid_0's auc: 0.797423\tvalid_0's binary_logloss: 0.574108\n",
      "[10000]\tvalid_0's auc: 0.797442\tvalid_0's binary_logloss: 0.573992\n",
      "Early stopping, best iteration is:\n",
      "[9971]\tvalid_0's auc: 0.797455\tvalid_0's binary_logloss: 0.574002\n",
      "Batch 9 started...\n",
      "[0]\tvalidation_0-auc:0.52099\n",
      "[200]\tvalidation_0-auc:0.66958\n",
      "[400]\tvalidation_0-auc:0.71388\n",
      "[600]\tvalidation_0-auc:0.73353\n",
      "[800]\tvalidation_0-auc:0.74848\n",
      "[1000]\tvalidation_0-auc:0.75975\n",
      "[1200]\tvalidation_0-auc:0.76760\n",
      "[1400]\tvalidation_0-auc:0.77357\n",
      "[1600]\tvalidation_0-auc:0.77921\n",
      "[1800]\tvalidation_0-auc:0.78319\n",
      "[2000]\tvalidation_0-auc:0.78638\n",
      "[2200]\tvalidation_0-auc:0.78846\n",
      "[2400]\tvalidation_0-auc:0.79020\n",
      "[2600]\tvalidation_0-auc:0.79120\n",
      "[2800]\tvalidation_0-auc:0.79219\n",
      "[3000]\tvalidation_0-auc:0.79293\n",
      "[3200]\tvalidation_0-auc:0.79351\n",
      "[3400]\tvalidation_0-auc:0.79407\n",
      "[3600]\tvalidation_0-auc:0.79453\n",
      "[3800]\tvalidation_0-auc:0.79493\n",
      "[4000]\tvalidation_0-auc:0.79525\n",
      "[4200]\tvalidation_0-auc:0.79552\n",
      "[4400]\tvalidation_0-auc:0.79581\n",
      "[4600]\tvalidation_0-auc:0.79602\n",
      "[4800]\tvalidation_0-auc:0.79621\n",
      "[5000]\tvalidation_0-auc:0.79639\n",
      "[5200]\tvalidation_0-auc:0.79658\n",
      "[5400]\tvalidation_0-auc:0.79673\n",
      "[5600]\tvalidation_0-auc:0.79686\n",
      "[5800]\tvalidation_0-auc:0.79697\n",
      "[6000]\tvalidation_0-auc:0.79713\n",
      "[6200]\tvalidation_0-auc:0.79722\n",
      "[6400]\tvalidation_0-auc:0.79733\n",
      "[6600]\tvalidation_0-auc:0.79742\n",
      "[6800]\tvalidation_0-auc:0.79746\n",
      "[7000]\tvalidation_0-auc:0.79755\n",
      "[7200]\tvalidation_0-auc:0.79763\n",
      "[7400]\tvalidation_0-auc:0.79765\n",
      "[7600]\tvalidation_0-auc:0.79772\n",
      "[7800]\tvalidation_0-auc:0.79778\n",
      "[8000]\tvalidation_0-auc:0.79785\n",
      "[8200]\tvalidation_0-auc:0.79788\n",
      "[8400]\tvalidation_0-auc:0.79789\n",
      "[8600]\tvalidation_0-auc:0.79792\n",
      "[8800]\tvalidation_0-auc:0.79791\n",
      "[8926]\tvalidation_0-auc:0.79791\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.674255\tvalid_0's binary_logloss: 0.679049\n",
      "[400]\tvalid_0's auc: 0.716407\tvalid_0's binary_logloss: 0.666295\n",
      "[600]\tvalid_0's auc: 0.739246\tvalid_0's binary_logloss: 0.655269\n",
      "[800]\tvalid_0's auc: 0.751372\tvalid_0's binary_logloss: 0.646524\n",
      "[1000]\tvalid_0's auc: 0.760589\tvalid_0's binary_logloss: 0.638904\n",
      "[1200]\tvalid_0's auc: 0.768078\tvalid_0's binary_logloss: 0.631953\n",
      "[1400]\tvalid_0's auc: 0.774064\tvalid_0's binary_logloss: 0.625647\n",
      "[1600]\tvalid_0's auc: 0.779339\tvalid_0's binary_logloss: 0.619814\n",
      "[1800]\tvalid_0's auc: 0.782387\tvalid_0's binary_logloss: 0.615291\n",
      "[2000]\tvalid_0's auc: 0.784926\tvalid_0's binary_logloss: 0.611192\n",
      "[2200]\tvalid_0's auc: 0.787229\tvalid_0's binary_logloss: 0.607342\n",
      "[2400]\tvalid_0's auc: 0.788934\tvalid_0's binary_logloss: 0.604004\n",
      "[2600]\tvalid_0's auc: 0.790385\tvalid_0's binary_logloss: 0.600957\n",
      "[2800]\tvalid_0's auc: 0.791542\tvalid_0's binary_logloss: 0.598296\n",
      "[3000]\tvalid_0's auc: 0.79242\tvalid_0's binary_logloss: 0.596064\n",
      "[3200]\tvalid_0's auc: 0.79312\tvalid_0's binary_logloss: 0.594067\n",
      "[3400]\tvalid_0's auc: 0.79377\tvalid_0's binary_logloss: 0.592248\n",
      "[3600]\tvalid_0's auc: 0.794249\tvalid_0's binary_logloss: 0.590524\n",
      "[3800]\tvalid_0's auc: 0.794838\tvalid_0's binary_logloss: 0.588815\n",
      "[4000]\tvalid_0's auc: 0.795294\tvalid_0's binary_logloss: 0.587298\n",
      "[4200]\tvalid_0's auc: 0.795689\tvalid_0's binary_logloss: 0.586009\n",
      "[4400]\tvalid_0's auc: 0.796049\tvalid_0's binary_logloss: 0.584802\n",
      "[4600]\tvalid_0's auc: 0.79628\tvalid_0's binary_logloss: 0.583822\n",
      "[4800]\tvalid_0's auc: 0.796528\tvalid_0's binary_logloss: 0.582891\n",
      "[5000]\tvalid_0's auc: 0.796755\tvalid_0's binary_logloss: 0.582038\n",
      "[5200]\tvalid_0's auc: 0.796954\tvalid_0's binary_logloss: 0.581284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5400]\tvalid_0's auc: 0.797155\tvalid_0's binary_logloss: 0.580566\n",
      "[5600]\tvalid_0's auc: 0.797296\tvalid_0's binary_logloss: 0.579927\n",
      "[5800]\tvalid_0's auc: 0.797381\tvalid_0's binary_logloss: 0.579426\n",
      "[6000]\tvalid_0's auc: 0.797505\tvalid_0's binary_logloss: 0.578875\n",
      "[6200]\tvalid_0's auc: 0.797639\tvalid_0's binary_logloss: 0.578361\n",
      "[6400]\tvalid_0's auc: 0.79773\tvalid_0's binary_logloss: 0.577874\n",
      "[6600]\tvalid_0's auc: 0.797839\tvalid_0's binary_logloss: 0.577461\n",
      "[6800]\tvalid_0's auc: 0.797952\tvalid_0's binary_logloss: 0.57704\n",
      "[7000]\tvalid_0's auc: 0.798089\tvalid_0's binary_logloss: 0.576614\n",
      "[7200]\tvalid_0's auc: 0.798137\tvalid_0's binary_logloss: 0.576289\n",
      "[7400]\tvalid_0's auc: 0.798175\tvalid_0's binary_logloss: 0.576002\n",
      "[7600]\tvalid_0's auc: 0.798191\tvalid_0's binary_logloss: 0.57576\n",
      "[7800]\tvalid_0's auc: 0.798241\tvalid_0's binary_logloss: 0.575515\n",
      "[8000]\tvalid_0's auc: 0.798259\tvalid_0's binary_logloss: 0.575288\n",
      "[8200]\tvalid_0's auc: 0.798321\tvalid_0's binary_logloss: 0.575052\n",
      "[8400]\tvalid_0's auc: 0.798359\tvalid_0's binary_logloss: 0.574853\n",
      "[8600]\tvalid_0's auc: 0.798443\tvalid_0's binary_logloss: 0.574627\n",
      "[8800]\tvalid_0's auc: 0.798469\tvalid_0's binary_logloss: 0.574461\n",
      "[9000]\tvalid_0's auc: 0.798512\tvalid_0's binary_logloss: 0.574293\n",
      "[9200]\tvalid_0's auc: 0.798536\tvalid_0's binary_logloss: 0.574148\n",
      "[9400]\tvalid_0's auc: 0.798547\tvalid_0's binary_logloss: 0.574014\n",
      "[9600]\tvalid_0's auc: 0.798553\tvalid_0's binary_logloss: 0.573883\n",
      "Early stopping, best iteration is:\n",
      "[9515]\tvalid_0's auc: 0.798559\tvalid_0's binary_logloss: 0.573936\n"
     ]
    }
   ],
   "source": [
    "for fold_index, (train_index,val_index) in enumerate(folds.split(X_train,y_train)):\n",
    "    print('Batch {} started...'.format(fold_index))\n",
    "    gc.collect()\n",
    "    bst = model_xgb.fit(X_train.iloc[train_index],y_train.iloc[train_index],\n",
    "              eval_set = [(X_train.iloc[val_index],y_train.iloc[val_index])],\n",
    "              early_stopping_rounds=200,\n",
    "              verbose= 200, \n",
    "              eval_metric ='auc')\n",
    "    bst_2 = model_lgb.fit(X_train.iloc[train_index],y_train.iloc[train_index],\n",
    "              eval_set = [(X_train.iloc[val_index],y_train.iloc[val_index])],\n",
    "              early_stopping_rounds=200,\n",
    "              verbose= 200, \n",
    "              eval_metric ='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x17e27de62b0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgb.booster_.save_model('lgb.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb.save_model('xgb.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GeForce RTX 2060 SUPER\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert training date to tensors\n",
    "X_nn = torch.FloatTensor(np.array(X_train))\n",
    "y_nn = torch.FloatTensor(np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=118, out_features=250, bias=True)\n",
      "  (fc2): Linear(in_features=250, out_features=150, bias=True)\n",
      "  (fc3): Linear(in_features=150, out_features=30, bias=True)\n",
      "  (fc4): Linear(in_features=30, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.15, inplace=False)\n",
      "  (bn1): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#model definition\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_nn.size(1), 250)\n",
    "        self.fc2 = nn.Linear(250, 150)\n",
    "        self.fc3 = nn.Linear(150, 30)\n",
    "        self.fc4 = nn.Linear(30, 1)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        self.bn1 = nn.BatchNorm1d(250)\n",
    "        self.bn2 = nn.BatchNorm1d(150)\n",
    "        self.bn3 = nn.BatchNorm1d(30)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "      \n",
    "        return x\n",
    "net = Net().to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape into tensor sequences for loading\n",
    "seq = []\n",
    "for i in range(0,len(X_nn)):\n",
    "    seq.append((X_nn[i],y_nn[i]))\n",
    "    i = i + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create trainloader\n",
    "trainloader = torch.utils.data.DataLoader(seq, batch_size=batch_size,shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set loss function and optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "steps = 30\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd0825bbef745e4b5e85976a080af2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6427841575387965\n",
      "0.6425306463942808\n",
      "0.6422869893796941\n",
      "0.6422884226801561\n",
      "0.6426763679573243\n",
      "0.6421121614661446\n",
      "0.6422560356039414\n",
      "0.6422819947813921\n",
      "0.6422955220077127\n",
      "0.6415933634348732\n",
      "0.6419891914263128\n",
      "0.6421304154682924\n",
      "0.6420572977015041\n",
      "0.641772390686892\n",
      "0.6416320574474844\n",
      "0.6414608428663111\n",
      "0.6419932916362018\n",
      "0.6413873504189884\n",
      "0.6416101664464104\n",
      "0.6417883289529678\n",
      "0.6420730563567921\n",
      "0.6414661457672476\n",
      "0.641509046050954\n",
      "0.6413614917247691\n",
      "0.6412351642222328\n",
      "0.6412284891395008\n",
      "0.6410008250551428\n",
      "0.6408863276402581\n",
      "0.6411242142399365\n",
      "0.6409949837999548\n",
      "0.6411924998071742\n",
      "0.6413379165737386\n",
      "0.6409047410927992\n",
      "0.6406813608133857\n",
      "0.6409062252325171\n",
      "0.6408582864438787\n",
      "0.6408453840304186\n",
      "0.6403545255170149\n",
      "0.6407980304510198\n",
      "0.6404540518866503\n",
      "0.6406765111627426\n",
      "0.6404620139675344\n",
      "0.6405891445390681\n",
      "0.6399952281605114\n",
      "0.640118390879529\n",
      "0.6403011780849752\n",
      "0.6400126533552924\n",
      "0.6405396684605807\n",
      "0.6400522549840856\n",
      "0.64021248310645\n",
      "0.6404015754155297\n",
      "0.6399094207401581\n",
      "0.6403769968665219\n",
      "0.6404051559972254\n",
      "0.6396749001295171\n",
      "0.6397138782204154\n",
      "0.6401660481398118\n",
      "0.6404534952844528\n",
      "0.6401131941991693\n",
      "0.6403415767266789\n",
      "0.6397295964752289\n",
      "0.6397426437566625\n",
      "0.6396651256052568\n",
      "0.6394257029906951\n",
      "0.6395239271422758\n",
      "0.6395737969460972\n",
      "0.6395932399334117\n",
      "0.6391964230467292\n",
      "0.6393431072885339\n",
      "0.6394889935612041\n",
      "0.6392807372432342\n",
      "0.6392046981316837\n",
      "0.6397381918953065\n",
      "0.639066431850673\n",
      "0.6395293305264437\n",
      "0.6393624218230579\n",
      "0.6392440478750728\n",
      "0.6392424552039029\n",
      "0.6394184007842273\n",
      "0.638779033314098\n",
      "0.6389559297797515\n",
      "0.6396010957937189\n",
      "0.6391810224177366\n",
      "0.6390732647104059\n",
      "0.6387060917316274\n",
      "0.6388271621523056\n",
      "0.6395628829212749\n",
      "0.6389402634797887\n",
      "0.6385002040608044\n",
      "0.6386479987180169\n",
      "0.6387089121947314\n",
      "0.6390168776008535\n",
      "0.6385084797035564\n",
      "0.6385469645421136\n",
      "0.638788407738196\n",
      "0.6385150043403401\n",
      "0.6382992230156527\n",
      "0.6381266205546691\n",
      "0.6383677124180258\n",
      "0.6383414500378033\n",
      "0.6386752109476589\n",
      "0.6385561874843536\n",
      "0.6384477709385163\n",
      "0.6386091341787481\n",
      "0.6383079552554829\n",
      "0.6382605462150778\n",
      "0.6378739170212159\n",
      "0.6380518555322433\n",
      "0.6385607663641639\n",
      "0.6379036651575629\n",
      "0.6379248008211666\n",
      "0.638287054344932\n",
      "0.6378900611145611\n",
      "0.6384543134248193\n",
      "0.6380575994916141\n",
      "0.6381881199577913\n",
      "0.6375733803619038\n",
      "0.6379542318098048\n",
      "0.6375660089885488\n",
      "0.6376546474701581\n",
      "0.6376556100692341\n",
      "0.6376712412279557\n",
      "0.6383624327852127\n",
      "0.6383936747032053\n",
      "0.6372989889453439\n",
      "0.6376663914179419\n",
      "0.6376738739523659\n",
      "0.6372900028279759\n",
      "0.6374288465887468\n",
      "0.6372749210997699\n",
      "0.6378651843829588\n",
      "0.6374335572681326\n",
      "0.6374277824705298\n",
      "0.637935326778315\n",
      "0.63737698441202\n",
      "0.6376681091154323\n",
      "0.6376219067503425\n",
      "0.6372754691756345\n",
      "0.6375776541264937\n",
      "0.6371151346733226\n",
      "0.6370794647995801\n",
      "0.6371384355473646\n",
      "0.6375756970542\n",
      "0.6373107007799301\n",
      "0.6369569393243382\n",
      "0.6373616331879468\n",
      "0.6374602806281279\n",
      "0.6370532903919883\n",
      "0.6366802365544008\n",
      "0.6373517189752609\n",
      "0.6370274574361383\n",
      "0.6373430339091601\n",
      "0.6371413203962346\n",
      "0.6368197709481347\n",
      "0.6371812159046132\n",
      "0.6369400826048723\n",
      "0.6369501027193937\n",
      "0.6370514304561411\n",
      "0.637150630514252\n",
      "0.6369982309998038\n",
      "0.6362689395639348\n",
      "0.6368141597445636\n",
      "0.6369934001549042\n",
      "0.6367892267551014\n",
      "0.6366947669396426\n",
      "0.6372331780864593\n",
      "0.6363311755784693\n",
      "0.6366465263825687\n",
      "0.6363402210614261\n",
      "0.6364401886488664\n",
      "0.6368012471313782\n",
      "0.6364543680998094\n",
      "0.6359287677282955\n",
      "0.6361312159401847\n",
      "0.6358765142167954\n",
      "0.6368248790980661\n",
      "0.6369687954691005\n",
      "0.6365458033779726\n",
      "0.6360907206401468\n",
      "0.6364346244596543\n",
      "0.6364086331690059\n",
      "0.6365274385334974\n",
      "0.6363010126001695\n",
      "0.6362543047111939\n",
      "0.6367803459817712\n",
      "0.6360848481483\n",
      "0.6364011445784952\n",
      "0.6362588702835501\n",
      "0.6362421099833626\n",
      "0.6360759949461025\n",
      "0.6362564671326448\n",
      "0.6358954366196923\n",
      "0.6362818792701405\n",
      "0.635901866909017\n",
      "0.636069069890415\n",
      "0.6362079677575412\n",
      "0.6361249935499487\n",
      "0.635945282199166\n",
      "0.6363710877410869\n",
      "0.636421613275686\n",
      "0.63557038388469\n",
      "0.636228995606861\n",
      "0.6357992319340374\n",
      "0.6362866987997198\n",
      "0.6361444451432815\n",
      "0.6365322657128706\n",
      "0.6357315664782244\n",
      "0.6354941827728149\n",
      "0.6360422126590249\n",
      "0.6354991548201617\n",
      "0.6355821847278167\n",
      "0.6359255197532674\n",
      "0.635891862412825\n",
      "0.6356290943801084\n",
      "0.6361965409574661\n",
      "0.6358374924583231\n",
      "0.6357249940302283\n",
      "0.6357830192316025\n",
      "0.6356748222986007\n",
      "0.6353716362606395\n",
      "0.6353723319137797\n",
      "0.6351037202353146\n",
      "0.6354692278698804\n",
      "0.6356989352460851\n",
      "0.6354832673933417\n",
      "0.635204477504613\n",
      "0.6356653794247836\n",
      "0.6353623597698416\n",
      "0.635805784460695\n",
      "0.6355111332819423\n",
      "0.6351127332863323\n",
      "0.6353044461279629\n",
      "0.6347617271909102\n",
      "0.6357849390908359\n",
      "0.6352456114030777\n",
      "0.6355670935967389\n",
      "0.6356538389137084\n",
      "0.6351785323517846\n",
      "0.6353633329670697\n",
      "0.6350628920098677\n",
      "0.6348385152651027\n",
      "0.6356171360787224\n",
      "0.6351735430924013\n",
      "0.6354985711249438\n",
      "0.6351142055529324\n",
      "0.6353333263314344\n",
      "0.6348651475447384\n",
      "0.6351455964028516\n",
      "0.6350180062061963\n",
      "0.634803787670671\n",
      "0.6351099389600244\n",
      "0.6350073190455768\n",
      "0.6348947070658526\n",
      "0.6348902834131118\n",
      "0.6349019749279328\n",
      "0.6348689339537034\n",
      "0.6349664177486604\n",
      "0.6347941798481712\n",
      "0.6349226048285949\n",
      "0.6345308579225591\n",
      "0.6351085239871938\n",
      "0.6346792702209503\n",
      "0.6348002879696096\n",
      "0.6347585058307903\n",
      "0.6348590899438145\n",
      "0.6347433972486201\n",
      "0.6349802230768663\n",
      "0.6348225318174311\n",
      "0.6352982916296485\n",
      "0.6347701337566988\n",
      "0.6345975952830544\n",
      "0.6347287976168057\n",
      "0.6348036745174683\n",
      "0.6348469536253475\n",
      "0.6347187096103627\n",
      "0.6349034589879653\n",
      "0.6338196588072548\n",
      "0.6342297957383375\n",
      "0.635014340839284\n",
      "0.6342359330246156\n",
      "0.6347226777019348\n",
      "0.6344497115376161\n",
      "0.6347692143470846\n",
      "0.6344320340590044\n",
      "0.6346809248873256\n",
      "0.6341900858968337\n",
      "0.6344558911368171\n",
      "0.6341932843075716\n",
      "0.6334559933387022\n",
      "0.634725586456411\n",
      "0.6350105109221158\n",
      "0.6342323640769816\n",
      "0.6344019329962246\n",
      "0.6351448417028641\n",
      "0.6342666681437569\n",
      "0.6341069695305697\n",
      "0.6344251039831396\n",
      "0.6343364620750601\n",
      "0.6342076494253893\n",
      "0.63405548490305\n",
      "0.6341081744528072\n",
      "0.6335534604794202\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-4252930a5675>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mrunning_val_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 801\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    802\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\multiprocessing\\reductions.py\u001b[0m in \u001b[0;36mreduce_storage\u001b[1;34m(storage)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreduce_storage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_sharing_strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot pickle CUDA storage; try pickling a CUDA tensor instead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "net.train().cuda()\n",
    "net.load_state_dict(torch.load('nn.pth'))\n",
    "\n",
    "#model training\n",
    "max_acc = 66.5\n",
    "min_loss = 1\n",
    "running_loss = 1\n",
    "running_val_loss = 1\n",
    "import time\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(500)):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    running_val_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, preds = data\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs.to(device))\n",
    "        loss = criterion(outputs, preds.to(device).reshape(-1,1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "    print(running_loss/(i + 1))\n",
    "print('Finished Training')\n",
    "print(\"time\",(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'nn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert training date to tensors\n",
    "X_nn_t = torch.FloatTensor(np.array(X_test))\n",
    "y_nn_t = torch.FloatTensor(np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5308725364736363"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval().cpu()\n",
    "roc_auc_score(y_test,torch.sigmoid(net(X_nn_t)).detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7974488009129919"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test,model_xgb.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7976420156248671"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test,model_lgb.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6359],\n",
       "        [0.6268],\n",
       "        [0.5177],\n",
       "        ...,\n",
       "        [0.6399],\n",
       "        [0.7442],\n",
       "        [0.4977]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(net(X_nn_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
